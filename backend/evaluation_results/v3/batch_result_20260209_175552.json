{
  "batch_id": "batch_20260209_173058",
  "timestamp": "2026-02-09T17:30:58.016381",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John emerged unscathed, but his best friend Tommy suffered a broken arm, and the bus driver was hospitalized.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 10.68474555015564,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7877237851662404,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9393939393939394,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.85,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9820248957019777,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.68474555015564
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The provided context mentions that a faulty wire in one of John's antique clocks caused the fire that destroyed his home.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 6.367434024810791,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5230769230769231,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4782608695652174,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8948366104634486,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.367434024810791
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "John Doe was twenty-one and newly employed as a junior assistant in the Evergreen Public Library. The head librarian, Mrs. Hargrove, called him into her office on a gray Thursday afternoon and informed him that his contract would not be renewed due to budget cuts. Three positions were being eliminated, including John's.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 7.234938621520996,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.12053571428571429,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.20689655172413793,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6428571428571429,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8221882198381822,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.234938621520996
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "The provided context does not contain information to answer this question. The story mentions a \"Wedding That Wasn't\" but only describes the events leading up to it and its cancellation, without providing any details about what actually happened during or after the event.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 7.18765926361084,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.014084507042253521,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0847457627118644,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4827586206896552,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.0625,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.5436854685315531,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.18765926361084
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How old was John when the school bus accident happened?",
      "expected_answer": "John was fifteen when the school bus accident occurred.",
      "generated_answer": "The provided context does not explicitly state John's age at the time of the school bus accident. However, it mentions that he was \"fifteen\" and describes him as a \"young teenager.\"",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 7.06529688835144,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.3389830508474576,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.11764705882352941,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8564115362744154,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.06529688835144
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What injury did Tommy suffer in the bus accident?",
      "expected_answer": "Tommy suffered a broken arm in the bus accident.",
      "generated_answer": "The provided context mentions that Tommy suffered a broken arm in the bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 6.473426342010498,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.732824427480916,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6923076923076923,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.5555555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9074738979528585,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.473426342010498
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John call the club he started after the bus accident?",
      "expected_answer": "John started a 'Bus Philosophy Club' after the bus accident.",
      "generated_answer": "The provided context does not contain information about what John called the club he started after the bus accident. However, it is mentioned that the club was a \"Bus Philosophy Club\" where members discussed existentialism over vending machine snacks.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 6.806570053100586,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1607717041800643,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.21621621621621623,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.731267727808852,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.806570053100586
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John use to dry his books after the bus accident?",
      "expected_answer": "John dried his books with a hairdryer after the bus accident.",
      "generated_answer": "A hairdryer.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 5.175732851028442,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.3287671232876712,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.18181818181818182,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.14285714285714285,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6244461459317259,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 5.175732851028442
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What profession did John have when his house burned down?",
      "expected_answer": "John worked as a librarian when his house burned down.",
      "generated_answer": "The provided context does not explicitly state what profession John had at the time of the fire. However, it mentions that he worked as a librarian and later refers to him working in an archive, but these details are from different parts of the narrative. The closest information is that he was a librarian when his house caught fire.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 7.6573216915130615,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.15979381443298968,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1568627450980392,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8554322751140118,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.6573216915130615
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John collect that caused the house fire?",
      "expected_answer": "John collected antique clocks, and a faulty wire in one caused the house fire.",
      "generated_answer": "The provided context does not explicitly state what specifically caused the house fire. However, it mentions that \"a faulty wire in one of his beloved clocks\" was the source of the fire. \n\nJohn collected antique clocks.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 6.817909240722656,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.20202020202020202,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3235294117647059,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8888888888888888,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9054769727508301,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.817909240722656
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do with the pigeons after the house fire?",
      "expected_answer": "John fed crumbs from a stale sandwich to pigeons after the house fire.",
      "generated_answer": "According to the context, after feeding them crumbs from a stale sandwich, John continued to feed pigeons in general, as mentioned later in the story. However, there is no specific information about what he did with the pigeons immediately after the house fire.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 6.205756187438965,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2175226586102719,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2894736842105263,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8535685907539132,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.205756187438965
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What type of clocks did John start collecting after the fire?",
      "expected_answer": "John started collecting digital clocks after the fire.",
      "generated_answer": "The provided context does not contain information about what type of clocks John started collecting before the fire. However, it mentions that after the fire, he started a new clock collection, this time digital ones that couldn't spark.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 6.655214786529541,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.23367697594501718,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.23529411764705882,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8502965268102731,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.655214786529541
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What book was John packing last in the archive?",
      "expected_answer": "John was packing a 17th-century edition of Montaigne\u2019s Essays last.",
      "generated_answer": "The provided context does not contain information to answer this question about which specific book John packed last in the archive. However, it mentions that he handled each volume as though saying goodbye to an old friend and opened a 17th-century edition of Montaigne's *Essays* on his final day, but it does not specify if this was the last book he packed or just one he read before sealing the cartons.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 8.228403568267822,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.18565400843881857,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0967741935483871,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7892159346088752,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.228403568267822
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What phrase was in the last book John packed?",
      "expected_answer": "The phrase 'Que s\u00e7ay-je?' (What do I know?) was in the last book.",
      "generated_answer": "The provided context mentions that inside the last carton John packed lay a 17th-century edition of Montaigne's *Essays*. The page he opened to read was:\n\n*\u201cQue s\u00e7ay-je?\u201d* \u2014 What do I know?",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 6.1782121658325195,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2992125984251969,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10526315789473684,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.14285714285714285,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7254339813216373,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.1782121658325195
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How long did John spend packing volumes in the archive?",
      "expected_answer": "John spent the last three weeks packing volumes in the archive.",
      "generated_answer": "The context mentions that John spent \"the last three weeks\" of the archive's open life packing volumes into acid-free boxes. It does not provide information on how many hours or days he worked during this time, only that it was a period of three weeks.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 7.1118738651275635,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.20317460317460317,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.16279069767441862,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7060215676829593,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.1118738651275635
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John keep from the archive as a private act?",
      "expected_answer": "John kept the call number of Montaigne\u2019s Essays as a private act.",
      "generated_answer": "The provided context does not contain information to answer this question fully. However, it mentions that John left the building carrying nothing but his coat and a single slip of paper he had slipped into his pocket: the call number of Montaigne's *Essays*. This can be inferred as a private act, but the specific item or items he kept from the archive are not explicitly stated in the provided context.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 7.664537668228149,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2553191489361702,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.75,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6844221247132056,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.664537668228149
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Who was the head librarian at Evergreen Public Library?",
      "expected_answer": "Mrs. Hargrove was the head librarian at Evergreen Public Library.",
      "generated_answer": "The provided context does not contain information about who the head librarian was before Mrs. Hargrove. It only mentions that John Doe's contract was not renewed by Mrs. Hargrove due to budget cuts, and she is referred to as \"the head librarian\" in this instance.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 7.1165807247161865,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.11550151975683891,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.13953488372093023,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7920486206609032,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.1165807247161865
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How many positions were eliminated at the library?",
      "expected_answer": "Three positions were eliminated at the library.",
      "generated_answer": "The provided context mentions that Mrs. Hargrove said, \"Three. Yours included.\" when John asked how many positions were being eliminated. \n\nTherefore, three positions were eliminated at the library.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 7.16987419128418,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.3836734693877551,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.28,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.812017040788178,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.16987419128418
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was the rare-book dealer's unusual requirement?",
      "expected_answer": "The rare-book dealer required a handwritten essay on why dust is noble.",
      "generated_answer": "The context mentions that the rare-book dealer required applicants to submit a handwritten essay on why dust is noble.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 6.611933469772339,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7513227513227513,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6666666666666666,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.5833333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9003968730164413,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.611933469772339
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John note about dust in his new job?",
      "expected_answer": "John decided that the dust was rather noble in his new job.",
      "generated_answer": "The provided context does not contain information about what John noted about dust in his new job. However, it is mentioned that as part of the application process for the rare-book dealer's cataloguing assistant position, applicants were required to submit a handwritten essay on why dust is noble.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 6.971661806106567,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.11731843575418995,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1702127659574468,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7387722859463468,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.971661806106567
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "John reacted calmly and philosophically to the school bus accident. He noted in his journal that \"Accidents are life's way of reminding us that control is an illusion. Why fight the farce?\" Instead of showing anger or sadness, he started a \"Bus Philosophy Club\" at school to discuss existentialism with his friends. When asked about the incident by a paramedic, John replied, \"Scary? It's just the universe playing bumper cars with reality.\" His calm demeanor stood out from others who were more emotional and reactive.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 8.300477027893066,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.021447721179624665,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1839080459770115,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5384615384615384,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7667383888931871,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.300477027893066
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John stood there, hands in his pockets, watching the smoke curl lazily into the sky like a sigh from the universe. He then said evenly, \"Well, it seems the house has decided to retire early. Can't blame it\u2014I've thought about it myself on Mondays.\" \n\nHe also pulled out a small notebook and jotted down: \"Note to self: Fire sales are literal.\"\n\nThis suggests that John reacted calmly and with a sense of detachment, observing the situation as if it were an ordinary event rather than a personal tragedy.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 8.086626768112183,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14444444444444443,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.35,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8694218141392135,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.086626768112183
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The provided context does not contain information to answer this question about why the university archive closed.\n\nHowever, it mentions that the university decided to digitize everything and close the physical stacks. The rare books would be moved to climate-controlled storage \u201cfor preservation.\u201d Access would be by appointment only, through high-resolution scans viewed on screens.\n\nAs for what book John was packing last, it is a 17th-century edition of Montaigne\u2019s *Essays*.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 8.278665542602539,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.3941176470588235,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2328767123287671,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6190476190476191,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8067485359152012,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.278665542602539
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "The provided context mentions that John received a class-action settlement notice related to a chemical plant leak into the river that supplied Evergreen's water. The notice offered modest compensation for \"potential long-term health effects.\" The amount is not specified, but it is enough for a new roof or a modest trip.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 7.3234453201293945,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.3420523138832998,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.75,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8777628099653992,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.3234453201293945
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "Mara was diagnosed with an aggressive form of leukemia. She died three weeks later, quietly, while John was reading aloud from a chapter on the indifference of the stars.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 6.703118085861206,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.4633431085043988,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3076923076923077,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.90625,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.55,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8897003009742646,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.703118085861206
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "The provided context does not contain information to answer this question about John's diagnosis at age 25, but it does mention his sister Mara's diagnosis with an aggressive form of leukemia. However, the text later mentions that John was diagnosed with pancreatic cancer at age 32.\n\nWhen he received the diagnosis, John listened quietly and asked only one question: \"How long until the coffee stops tasting like coffee?\" He then left the office carrying a pamphlet titled *Living with Advanced Illness*.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 8.258495807647705,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.07092198581560284,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18292682926829268,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8156476537848668,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.258495807647705
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do the day after losing his library job?",
      "expected_answer": "The day after losing his job, John sat at his desk and wrote notes, then planned to apply for three more jobs, one of which he expected to be ridiculous.",
      "generated_answer": "The provided context does not contain information about what John did the day after losing his library job in \"The Library of Small Disappointments\". It mentions that he left the office and spent the rest of his shift reshelving returns, but it does not specify his activities for the next day.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 7.378498315811157,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1476510067114094,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1724137931034483,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.1875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7275483032533039,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.378498315811157
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did Mara die and what was John's immediate action?",
      "expected_answer": "Mara died quietly while John was reading to her from a chapter on the indifference of the stars. He finished the paragraph before closing the book.",
      "generated_answer": "The provided context does not contain information on how Mara died. However, it mentions that she died three weeks later, quietly, while John was reading aloud from a chapter on the indifference of the stars.\n\nAs for John's immediate action after Mara's death, he sat with her for a long time, listening to the machines wind down into silence.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 6.715457916259766,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.35918367346938773,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.288135593220339,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8621054004032571,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.16666666666666666,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 6
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.715457916259766
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say about fear to Mara?",
      "expected_answer": "John said to Mara that fear is just the body\u2019s last honest opinion, but she would find out whether the absurd has an ending or just another act.",
      "generated_answer": "John said to Mara, \"Fear is just the body's last honest opinion. But look at it this way: you're about to find out whether the absurd has an ending or just another act. Either way, you win the argument with everyone who ever claimed life makes sense.\"",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 7.453738212585449,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.3189873417721519,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.38,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6785714285714286,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7777777777777778,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8324728370406173,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.125,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 8
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.453738212585449
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do with Mara's hospital bracelet?",
      "expected_answer": "John added Mara's hospital bracelet to his mementos box labeled 'Act III' after her death.",
      "generated_answer": "He added it to a small wooden box where he kept mementos and labeled it \"Act III\".",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 6.598955869674683,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.4883720930232558,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.1333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2727272727272727,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.781473772137816,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.598955869674683
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did the oncologist describe John's prognosis?",
      "expected_answer": "The oncologist described John's prognosis as statistics that hovered between 'slim' and 'theoretical' for stage IV pancreatic cancer.",
      "generated_answer": "The context mentions that the oncologist spoke in measured sentences, offering statistics that hovered somewhere between \u201cslim\u201d and \u201ctheoretical.\u201d",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 6.626381158828735,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5591397849462365,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0555555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.38461538461538464,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7752757852099514,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.626381158828735
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What adjustments did John make after his diagnosis?",
      "expected_answer": "John adjusted his routine by fifteen minutes each morning to accommodate the new fatigue and continued his routines with minor adjustments.",
      "generated_answer": "The provided context mentions that John adjusted his routine by fifteen minutes each morning to accommodate the new fatigue that arrived due to his pancreatic cancer diagnosis.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 6.680435419082642,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7047619047619048,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4827586206896552,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6428571428571429,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8640071493518628,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.680435419082642
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What note did John leave in his cupboard?",
      "expected_answer": "John left a note saying: 'If you\u2019re reading this, the coffee is probably stale. Make a new pot anyway. Life is absurd enough without drinking yesterday\u2019s grounds.'",
      "generated_answer": "The provided context mentions that Elena found a single sheet of paper taped inside the cupboard above the coffee maker with John's neat handwriting:\n\n\"If you\u2019re reading this, the coffee is probably stale. Make a new pot anyway. Life is absurd enough without drinking yesterday\u2019s grounds.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 6.945653200149536,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.6371681415929203,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.41304347826086957,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7037037037037037,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7894736842105263,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.900716084983699,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.945653200149536
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do after the wedding was called off?",
      "expected_answer": "John went to the church, waited twenty minutes for a miracle, then walked out, bought a pretzel, ate half, and fed the rest to pigeons.",
      "generated_answer": "After the wedding was called off, John stood outside St. Agnes Church holding a single white rose he had purchased for Lila. He waited for twenty minutes in case she changed her mind, but when no one arrived, he walked out into bright noon sunlight and met the street vendor again. Instead of giving him the rose back, John bought a pretzel from the vendor and ate half of it on the walk home, feeding the rest to pigeons who had no opinion on weddings, broken or otherwise.\n\nLater that day, John wandered to the local park, sat on a bench, and fed pigeons crumbs from a stale sandwich. He mused aloud about life being like a library where books burn but stories float around, waiting for new shelves.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 9.463480472564697,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1674641148325359,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1308411214953271,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8098741146463919,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.463480472564697
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say to the vendor about the rose?",
      "expected_answer": "John said the rose worked perfectly, as it reminded him that beauty doesn\u2019t always come with permanence.",
      "generated_answer": "The provided context mentions that when the street vendor asked if \"the rose didn't work out,\" John replied, \"It worked perfectly. It just reminded me that beauty doesn\u2019t always come with permanence.\"",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 7.090843915939331,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.46710526315789475,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.34285714285714286,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8823529411764706,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8461538461538461,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8504990673698831,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.090843915939331
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John keep from the archive and why?",
      "expected_answer": "John kept a slip of paper with the call number of Montaigne\u2019s Essays as a small, private act of defiance against total digitization.",
      "generated_answer": "The provided context does not contain information on what exactly John kept from the archive. However, it mentions that he slipped a single slip of paper into his pocket with the call number of Montaigne's *Essays*. This suggests that he may have kept this specific piece of information or perhaps the book itself, but the text does not explicitly state what he took.\n\nIt is also worth noting that in \"The Uninvited Guest,\" John mentions adjusting his routine to accommodate his new fatigue and still cataloging fragile first editions. However, there is no mention of him taking anything from the archive with him.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 8.42151951789856,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1447721179624665,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.11627906976744186,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6412657391334143,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.6,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.42151951789856
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say about missing the books?",
      "expected_answer": "John said missing is just another way of remembering, and remembering is free.",
      "generated_answer": "John said, \"I already do\" (missing the books), but then added, \"But missing is just another way of remembering. And remembering is free.\"",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 7.448267459869385,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7162790697674418,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.36,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7692307692307692,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.863197097008968,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.448267459869385
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John use the settlement money for?",
      "expected_answer": "John used the settlement money to buy a better coffee maker.",
      "generated_answer": "According to the context in \"The Ordinary Tuesday,\" John read the amount of the settlement and thought it was enough for a new roof or a modest trip. However, he didn't specify what he actually used the money for. The provided context does not contain information on how John spent the settlement money.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 7.805935621261597,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.12121212121212122,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14893617021276595,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6651897698010816,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.805935621261597
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John think about fighting entropy?",
      "expected_answer": "John thought fighting entropy is like yelling at gravity\u2014undignified.",
      "generated_answer": "According to the context, when Mr. Patel asked if John would fight the settlement notice (the \"invoice\" from the universe), John replied, \"No point. Fighting entropy is like yelling at gravity. Undignified.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 6.880851984024048,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.45652173913043476,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.22580645161290322,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8103697720432406,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.14285714285714285,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 7
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 6.880851984024048
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John cope with the house fire and what metaphor did he use?",
      "expected_answer": "John coped by accepting it calmly, using the metaphor that life's a library where books burn but stories float around waiting for new shelves.",
      "generated_answer": "According to the context, John coped with the house fire by being \"merely present\" and not distraught. He observed the ruins calmly and even made a note in his journal about it.\n\nHe used the metaphor of a library to describe life: \"Life's a library,\" he mused aloud. \"Books burn, but the stories? They float around, waiting for new shelves.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 7.659234285354614,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.12008281573498965,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1935483870967742,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7058823529411765,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8152957258376465,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.659234285354614
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "According to the provided context, after losing his library job due to budget cuts, John applied for three new positions:\n\n1. Night security at a self-storage facility.\n2. Part-time barista at a shop that only served oat-milk lattes.\n3. Cataloguing assistant at a private rare-book dealer who insisted applicants submit a handwritten essay on why dust is noble.\n\nJohn took the third job, which was cataloguing assistant at the private rare-book dealer.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 8.709029912948608,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.37175792507204614,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.421875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8461538461538463,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.84,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8484978898900754,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.709029912948608
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "John handled his sister's death by being present with her in the hospital, reading to her from old philosophy books, and sitting with her until she passed away. He also took care of himself after her death, returning to his apartment above the laundromat and adding Mara's hospital bracelet to a small wooden box where he kept mementos, labeling it \"Act III\".",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 8.00517463684082,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02882882882882883,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18309859154929578,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7714285714285714,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8510207406413886,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.00517463684082
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "According to the provided context, John's last requests were:\n\n1. A fresh pot of truly terrible coffee.\n2. That someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.\n\nThese requests were made when the hospice nurse asked if he had any last wishes.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 7.392704010009766,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.568075117370892,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3953488372093023,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7785771858436414,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.392704010009766
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "Throughout his life, John's philosophy about tragedy was that it is an inevitable and natural part of life, akin to a misplaced comma in a sentence or the universe's way of editing the plot. He believed that tragedies are not something to be feared or raged against, but rather accepted and even found absurdity in. He often compared life to a library where books burn, but the stories continue to float around, waiting for new shelves. John's philosophy was one of calm acceptance, stoicism, and finding meaning in the midst of chaos.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 8.154955387115479,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02346805736636245,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.16666666666666666,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.34782608695652173,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8113299336276154,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.6,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.154955387115479
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "According to the provided context:\n\n1. Junior assistant at the Evergreen Public Library (at age 21)\n2. Cataloguing assistant at a private rare-book dealer (after applying for three jobs, including night security and barista positions; no specific age mentioned)\n3. Part-time job as a barista at a shop that only served oat-milk lattes (no specific age mentioned)\n4. Night security at a self-storage facility (no specific age mentioned)\n5. Cataloguing assistant at the university archive (after graduating with a degree in library science; no specific age mentioned)\n\nNote: The context does not provide an exhaustive list of John's jobs throughout his life, but these are the ones mentioned specifically.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 9.00448727607727,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1172566371681416,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.13636363636363635,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.55,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6305633087430687,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 4,
            "expected_count": 4,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt",
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.00448727607727
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.7333333333333334,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's reactions to the bus accident and house fire reflect his philosophy?",
      "expected_answer": "In both the bus accident and house fire, John reacted with philosophical detachment and calm acceptance, seeing them as the universe's absurd events rather than personal tragedies, starting a philosophy club after the accident and rebuilding deliberately after the fire.",
      "generated_answer": "John's reactions to both the bus accident and the house fire reflected his philosophy of embracing the absurdity and unpredictability of life. In both situations, he remained calm and detached, viewing the events as mere \"plot twists\" or \"punchlines\" in an ongoing narrative.\n\nWhen the bus skidded on wet pavement and collided with a tree, John observed that it was just \"the universe playing bumper cars with reality.\" He didn't express fear or anger but instead noted that his friend's broken arm would mend like a plot twist in a bad novel. Similarly, when his house caught fire from a faulty wire, he saw it as the house deciding to retire early and joked about fire sales being literal.\n\nIn both cases, John demonstrated an acceptance of life's unpredictability and a willingness to find humor or absurdity in tragic situations. His reactions suggest that he has internalized the idea that control is an illusion and that one should not fight against the chaos of life but rather learn to navigate it with equanimity.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 10.41967511177063,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.018575851393188854,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.11363636363636363,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3793103448275862,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.856336107491084,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.41967511177063
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What common theme connects John's job loss at the library and the archive closure?",
      "expected_answer": "Both events involved budget cuts or modernization leading to job changes; John lost his junior assistant role to cuts and the archive closed for digitization, but he adapted by finding new roles in rare books and eventually owning a bookstore.",
      "generated_answer": "The provided context does not explicitly state what connects John's job loss at the library and the archive closure. However, it can be inferred that both events involve a sense of loss or disruption in John's life.\n\nIn \"The Library of Small Disappointments,\" John loses his part-time job as a junior assistant due to budget cuts. He takes this news calmly, viewing it as an inconvenience rather than a personal failure.\n\nIn \"The Last Book,\" the archive where John works is closed and its physical collections are digitized. This event marks the end of an era for John, who has spent years handling rare books and developing a deep connection with them.\n\nA common theme that connects these two events is John's ability to cope with loss and change in a detached, philosophical manner. He views both job loss and archive closure as inevitable parts of life, rather than personal tragedies. This theme is consistent with his character's stoic and introspective personality.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt"
      ],
      "response_time": 10.215925455093384,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08072487644151564,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15873015873015872,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.48,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8118760892930739,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.2222222222222222,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.215925455093384
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's handling of his sister's death compare to his own cancer diagnosis?",
      "expected_answer": "John handled both with stoicism and calm; he finished reading to Mara and labeled her bracelet 'Act III,' and for his cancer, he adjusted routines minimally and saw it as a predictable plot.",
      "generated_answer": "The provided context does not directly compare John's handling of his sister's death and his own cancer diagnosis. However, it mentions that when his sister Mara was diagnosed with leukemia, he \"did not cry. He did not pace. He simply sat at his small kitchen table, listening to the laundromat's rhythmic thump-thump below, and thought: *Well. The wheel keeps turning.*\"\n\nIn contrast, when John received his own diagnosis of pancreatic cancer, he asked only one question: \"How long until the coffee stops tasting like coffee?\" He then adjusted his routine by fifteen minutes each morning to accommodate the new fatigue.\n\nThe context does not provide a direct comparison between the two situations.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 8.3800368309021,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03153153153153153,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10476190476190476,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2631578947368421,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6426683975136405,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.4,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.3800368309021
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What role did coffee play in John's life across events?",
      "expected_answer": "Coffee was a constant ritual; he made terrible coffee during his cancer, requested terrible coffee as a last wish, and used settlement money for a better coffee maker, symbolizing normalcy amid tragedy.",
      "generated_answer": "The provided context mentions that:\n\n* In \"The Uninvited Guest,\" John asks how long until the coffee stops tasting like coffee after being diagnosed with pancreatic cancer.\n* He still brews terrible instant coffee in the staff break room at work and accepts food from friends, including lasagna, which he considers while discussing his emotions with Elena.\n* When dying, he requests a fresh pot of truly terrible coffee as one of his last wishes.\n* In \"The Ordinary Tuesday,\" John makes coffee that is exactly the right shade of bitter every morning and brews fresh coffee for customers at his bookstore.\n\nOverall, coffee plays a consistent role in John's life across events, often serving as a mundane aspect of his daily routine.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 9.122990369796753,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.09003215434083602,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08490566037735849,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.30434782608695654,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8191728600855508,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.8,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.122990369796753
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's wedding cancellation and job losses show his resilience?",
      "expected_answer": "John treated the wedding cancellation like a plot twist, feeding pigeons afterward, and after job losses from budget cuts and archive closure, he applied for new roles and adapted without distress.",
      "generated_answer": "The provided context does not contain information about how John's wedding cancellation and job losses showed his resilience. It mentions these events as part of his life experiences but does not explicitly discuss their impact on his resilience.\n\nHowever, it can be inferred from the text that John's calm and matter-of-fact reactions to these setbacks demonstrate a certain level of emotional resilience. For example, when Lila cancels their wedding, he says \"I understand\" and waits at the church for twenty minutes before leaving. When he loses his job at the library, he takes it in stride and applies for new positions.\n\nThe text also suggests that John's ability to observe and comment on life's disappointments without becoming overly emotional may be a sign of resilience. For instance, when talking about the rose he bought for Lila, he says \"It worked perfectly... It just reminded me that beauty doesn\u2019t always come with permanence.\" This kind of philosophical detachment could indicate that John has learned to cope with disappointment and uncertainty in his life.\n\nHowever, it is essential to note that these interpretations are based on inference rather than explicit statements about resilience.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt"
      ],
      "response_time": 11.13947868347168,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.06534090909090909,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09027777777777778,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2727272727272727,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6628383560165957,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.5555555555555556,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.1,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.13947868347168
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What environmental event linked to John's settlement, and how did it tie to his health?",
      "expected_answer": "The chemical plant leak into Evergreen's water supply led to the settlement for potential health effects, which subtly connected to his later cancer diagnosis, though he accepted both without fury.",
      "generated_answer": "The provided context mentions that a chemical plant upstream had leaked into the river that supplied Evergreen's water. This leak is linked to John's settlement, as he received notice of a class-action settlement for \"potential long-term health effects\" due to the contamination.\n\nHowever, it does not fully explain how this event ties directly to his health.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 7.607722520828247,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13309352517985612,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18461538461538463,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8666666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4090909090909091,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8437806760352471,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.607722520828247
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's philosophy evolve from adolescence to death?",
      "expected_answer": "From seeing accidents as 'bumper cars with reality' in adolescence, to accepting fire as 'editing the plot,' to viewing his cancer as a 'predictable plot' with curiosity about the ending, John consistently embraced life's absurdity.",
      "generated_answer": "The provided context does not contain a comprehensive explanation of how John's philosophy evolved over time. However, it can be inferred that his views remained consistent in their absurd and stoic nature.\n\nIn adolescence, John was described as navigating life like a philosopher lost in a video arcade, with a penchant for reading Camus and viewing the world as a \"peculiar exhibit in a museum of mishaps.\" He saw accidents and tragedies as mere footnotes in an endless, ridiculous manuscript.\n\nAs he grew older, his philosophy seemed to solidify. When Mara was dying, John told her that fear is just the body's last honest opinion, but that she would soon find out whether the absurd has an ending or just another act. He also said that life makes sense only if you're willing to accept its absurdity.\n\nIn adulthood, when faced with his own terminal illness, John continued to apply his philosophy. He saw himself as \"just the audience\" watching the body die, and described the plot of his story as predictable but with an uncertain intermission length.\n\nThroughout his life, John's philosophy remained centered on accepting and even finding humor in the absurdity of life, rather than resisting or fighting it.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 11.287872552871704,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02074688796680498,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.32,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7466871018336814,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.5,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.287872552871704
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What mementos did John keep from family tragedies?",
      "expected_answer": "From his sister's death, John kept her hospital bracelet in his 'Act III' mementos box; tragedies like parents' accident and sister's death shaped his stoic philosophy.",
      "generated_answer": "The provided context does not contain information about what mementos John kept from family tragedies before his house caught fire. However, after Mara's death, he added her hospital bracelet to the small wooden box, labeling it \"Act III\".",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 7.356405019760132,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08353808353808354,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4615384615384615,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.45,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7896690682993647,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 7.356405019760132
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did books and libraries thread through John's career setbacks?",
      "expected_answer": "John's library job loss led to rare-book cataloguing, then university archive work ending in digitization, culminating in owning a bookstore; books were constant amid changes.",
      "generated_answer": "The provided context does not contain information to answer this question comprehensively. However, it mentions that:\n\n* John worked as a junior assistant in the Evergreen Public Library when his contract was not renewed due to budget cuts.\n* He later took a job as a cataloguing assistant at a private rare-book dealer.\n* The archive where he worked suffered its final indignity: the university decided to digitize everything and close the physical stacks.\n\nThe context also mentions John's philosophical views on books and libraries, but it does not provide a detailed account of how these institutions threaded through his career setbacks.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 8.446293115615845,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03916768665850673,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10989010989010989,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3684210526315789,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7917127890634073,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.446293115615845
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What ironies did John note in his life events?",
      "expected_answer": "Ironies like a clock causing a fire, digitization closing an archive of physical books, and a water pollution settlement possibly linking to his cancer, all accepted as life's absurdities.",
      "generated_answer": "John noted several ironies in his life events:\n\n1. \"Accidents are life's way of reminding us that control is an illusion. Why fight the farce?\" (after his house caught fire)\n2. \"Even metal bends to chaos. Might as well laugh at the bend.\" (about the dented locker, but also applicable to life's tragedies)\n3. \"Books burn, but the stories? They float around, waiting for new shelves.\" (after his home and belongings were destroyed in a fire)\n4. \"Tragedy is just the universe's way of editing the plot. Why rage against the red pen?\" (about coping with adversity)\n\nThese statements suggest that John finds irony and absurdity in life's challenges, often viewing them as opportunities for growth or perspective rather than sources of distress.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 9.314934968948364,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02586206896551724,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.07894736842105263,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.10526315789473684,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7420008377727125,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.16666666666666666,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 4,
            "expected_count": 4,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt",
              "the-last-book.txt",
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.5,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.314934968948364
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.7333333333333334,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's relationships with friends and family show his detachment?",
      "expected_answer": "John offered observations instead of emotions to friends during cancer, was stoic with Mara during her death, and calmly accepted Lila's wedding cancellation.",
      "generated_answer": "The provided context does not contain information to answer this question comprehensively. However, it mentions that:\n\n* When asked how he coped so calmly after the school bus accident, John would shrug and say \"Tragedy is just the universe's way of editing the plot. Why rage against the red pen?\"\n* His parents fretted when he got home, expecting tears or tantrums, but instead, John methodically dried his books with a hairdryer.\n* When his friend Elena asked how he was doing it, John replied, \"I'm not... The body is doing the dying part. I'm just the audience.\"\n* In general, John's responses to tragedy and adversity are characterized by calmness, detachment, and philosophical observations.\n\nThese instances suggest that John's relationships with friends and family may be affected by his detached attitude towards life's challenges. However, without more information on how others perceive or interact with him in these situations, it is difficult to provide a more detailed answer.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 9.335930585861206,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.017406440382941687,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.06153846153846154,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.643790646885303,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.5,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.335930585861206
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What common items symbolized normalcy in John's tragedies?",
      "expected_answer": "Coffee symbolized normalcy: terrible coffee during cancer, a new maker from settlement, and pretzels/pigeons after wedding, all grounding him in routine.",
      "generated_answer": "The provided context does not explicitly state what common items symbolized normalcy in John's tragedies. However, it mentions that:\n\n* In the first tragedy (the house fire), John was feeding pigeons crumbs from a stale sandwich he'd forgotten in his car.\n* In the second tragedy (the school bus accident), John had a soggy sandwich in his bag.\n\nThese sandwiches seem to be mentioned as mundane items that John is preoccupied with, but it's unclear if they specifically symbolize normalcy.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 8.211308240890503,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08722741433021806,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.04938271604938271,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.13333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7802492843280554,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.14285714285714285,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 7
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.3,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.211308240890503
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did Evergreen feature in John's life story?",
      "expected_answer": "Evergreen was the suburb of his adolescence, home to his library job, water pollution settlement, and where he owned his bookstore amid various tragedies.",
      "generated_answer": "The provided context does not contain information to answer this question comprehensively, but here are some key points about how Evergreen featured in John's life story:\n\n* Evergreen is mentioned as the town where John lived.\n* In \"The Calm Storm of John Doe,\" John's home in Evergreen caught fire due to a faulty wire in one of his clocks. The house was destroyed, leaving John with nothing but his small notebook and a sense of calm.\n* In \"The Absurd Adolescence of John Doe,\" the school bus accident occurred on a rainy afternoon in Evergreen. John emerged unscathed from the incident, which shook the community.\n* In \"The Quiet Clockwork of John Doe,\" John lived above a laundromat in Evergreen and worked part-time cataloguing rare books at the university archive.\n\nHowever, there are gaps in the information about how Evergreen specifically impacted or influenced John's life beyond these events.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 9.952309370040894,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03405865657521287,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.07964601769911504,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6983448387710133,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.952309370040894
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What role did reading play in John's coping mechanisms?",
      "expected_answer": "John read philosophy to Mara, continued reading during chemo, and packed Montaigne's Essays last, using books to process absurdity and mortality.",
      "generated_answer": "Reading played a significant role in John's coping mechanisms. He often read to Mara from old philosophy books when she was hospitalized and dying. After her death, he continued to read, finding solace in the words of philosophers like Camus. Even during his own illness, he would read in hospital recliners, watching dust motes float through slanted afternoon light.\n\nReading seemed to provide John with a sense of calm and perspective, helping him navigate life's challenges and tragedies. He used reading as a way to find meaning and make sense of the absurdity of life.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt"
      ],
      "response_time": 8.353174209594727,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.10863509749303621,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.12048192771084337,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.47058823529411764,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8144046814505921,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 8.353174209594727
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's possessions reflect his philosophy?",
      "expected_answer": "Clocks (causing fire), mementos box ('Act III'), philosophy books, and coffee rituals all embodied acceptance of time, memory, and life's indifference.",
      "generated_answer": "The provided context does not contain information about how John's possessions reflected his philosophy. However, it mentions that he kept a small wooden box with mementos, including Mara's hospital bracelet labeled \"Act III.\" It also describes his apartment above the laundromat and his bookstore as ordinary and unremarkable, suggesting that he values simplicity and does not attach great importance to material possessions.\n\nLater in the text, it is mentioned that John started a new clock collection after the previous one sparked. This could be seen as an example of how his philosophy influences his actions, but more context would be needed to fully understand this point.\n\nIt can also be noted that John's behavior and reactions to various situations are described as calm and accepting, which reflects his philosophical outlook on life.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 9.398261070251465,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03614457831325301,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.034782608695652174,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.125,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6770864992228584,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.1111111111111111,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.5,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.398261070251465
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 60,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.0,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 0.0,
    "AnswerEvaluator_exact_match_count": 60,
    "AnswerEvaluator_sequence_similarity_mean": 0.24721689523494378,
    "AnswerEvaluator_sequence_similarity_min": 0.014084507042253521,
    "AnswerEvaluator_sequence_similarity_max": 0.7877237851662404,
    "AnswerEvaluator_sequence_similarity_count": 60,
    "AnswerEvaluator_word_overlap_mean": 0.22221767503196999,
    "AnswerEvaluator_word_overlap_min": 0.034782608695652174,
    "AnswerEvaluator_word_overlap_max": 0.75,
    "AnswerEvaluator_word_overlap_count": 60,
    "AnswerEvaluator_answer_length_ratio_mean": 1.820055986564396,
    "AnswerEvaluator_answer_length_ratio_min": 0.18181818181818182,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 60,
    "AnswerEvaluator_keyword_match_mean": 0.5580428444813349,
    "AnswerEvaluator_keyword_match_min": 0.0625,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 60,
    "AnswerEvaluator_semantic_similarity_mean": 0.7906892451895305,
    "AnswerEvaluator_semantic_similarity_min": 0.5436854685315531,
    "AnswerEvaluator_semantic_similarity_max": 0.9820248957019777,
    "AnswerEvaluator_semantic_similarity_count": 60,
    "AnswerEvaluator_faithfulness_mean": 0.38231481481481483,
    "AnswerEvaluator_faithfulness_min": 0.0,
    "AnswerEvaluator_faithfulness_max": 1.0,
    "AnswerEvaluator_faithfulness_count": 60,
    "RetrievalEvaluator_context_match_mean": 1.0,
    "RetrievalEvaluator_context_match_min": 1.0,
    "RetrievalEvaluator_context_match_max": 1.0,
    "RetrievalEvaluator_context_match_count": 60,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8721230158730161,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.125,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 1.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 60,
    "RetrievalEvaluator_recall_at_3_mean": 0.8180555555555554,
    "RetrievalEvaluator_recall_at_3_min": 0.0,
    "RetrievalEvaluator_recall_at_3_max": 1.0,
    "RetrievalEvaluator_recall_at_3_count": 60,
    "RetrievalEvaluator_recall_at_5_mean": 0.8666666666666666,
    "RetrievalEvaluator_recall_at_5_min": 0.0,
    "RetrievalEvaluator_recall_at_5_max": 1.0,
    "RetrievalEvaluator_recall_at_5_count": 60,
    "RetrievalEvaluator_recall_at_10_mean": 0.9694444444444444,
    "RetrievalEvaluator_recall_at_10_min": 0.3333333333333333,
    "RetrievalEvaluator_recall_at_10_max": 1.0,
    "RetrievalEvaluator_recall_at_10_count": 60,
    "RetrievalEvaluator_precision_at_10_mean": 0.23666666666666675,
    "RetrievalEvaluator_precision_at_10_min": 0.1,
    "RetrievalEvaluator_precision_at_10_max": 0.6,
    "RetrievalEvaluator_precision_at_10_count": 60,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 60,
    "PerformanceEvaluator_retrieval_waste_mean": 0.9011111111111103,
    "PerformanceEvaluator_retrieval_waste_min": 0.7333333333333334,
    "PerformanceEvaluator_retrieval_waste_max": 0.9333333333333333,
    "PerformanceEvaluator_retrieval_waste_count": 60,
    "PerformanceEvaluator_answer_provided_mean": 1.0,
    "PerformanceEvaluator_answer_provided_min": 1.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 60,
    "per_difficulty": {
      "easy": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.3065478710786023,
        "AnswerEvaluator_word_overlap_mean": 0.27029049044324543,
        "AnswerEvaluator_answer_length_ratio_mean": 1.7571429815395334,
        "AnswerEvaluator_keyword_match_mean": 0.7010813492063491,
        "AnswerEvaluator_semantic_similarity_mean": 0.7987718648335295,
        "AnswerEvaluator_faithfulness_mean": 0.4541666666666667,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 1.0,
        "RetrievalEvaluator_recall_at_3_mean": 1.0,
        "RetrievalEvaluator_recall_at_5_mean": 1.0,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_10_mean": 0.17000000000000004,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.9333333333333333,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "medium": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.33602919822584504,
        "AnswerEvaluator_word_overlap_mean": 0.2534484084013745,
        "AnswerEvaluator_answer_length_ratio_mean": 1.755735600864277,
        "AnswerEvaluator_keyword_match_mean": 0.5904272118919951,
        "AnswerEvaluator_semantic_similarity_mean": 0.8112655060946633,
        "AnswerEvaluator_faithfulness_mean": 0.27999999999999997,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8467261904761905,
        "RetrievalEvaluator_recall_at_3_mean": 0.85,
        "RetrievalEvaluator_recall_at_5_mean": 0.85,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_10_mean": 0.17000000000000007,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.9333333333333333,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "hard": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.09907361640038412,
        "AnswerEvaluator_word_overlap_mean": 0.14291412625129002,
        "AnswerEvaluator_answer_length_ratio_mean": 1.9472893772893773,
        "AnswerEvaluator_keyword_match_mean": 0.3826199723456595,
        "AnswerEvaluator_semantic_similarity_mean": 0.7620303646403984,
        "AnswerEvaluator_faithfulness_mean": 0.4127777777777778,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7696428571428571,
        "RetrievalEvaluator_recall_at_3_mean": 0.6041666666666666,
        "RetrievalEvaluator_recall_at_5_mean": 0.7499999999999999,
        "RetrievalEvaluator_recall_at_10_mean": 0.9083333333333334,
        "RetrievalEvaluator_precision_at_10_mean": 0.37,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.8366666666666671,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      }
    },
    "success_rate": 1.0,
    "avg_response_time": 7.8552305698394775
  }
}