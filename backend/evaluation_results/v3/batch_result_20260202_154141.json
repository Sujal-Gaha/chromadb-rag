{
  "batch_id": "batch_20260202_152937",
  "timestamp": "2026-02-02T15:29:37.224290",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The bus skidded on wet pavement and crashed into a fallen tree. The impact caused screams, shattered glass and burnt\u2011rubber fumes. John survived without injury, but his best friend Tommy broke an arm and the bus driver was hospitalized.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 28.516472339630127,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.21076233183856502,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4318181818181818,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.1818181818181819,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.516472339630127
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "John reacted with calm, philosophical detachment, unlike the anxious, tear\u2011filled responses of others. While parents hovered, counselors offered group hugs, and many replayed the horror in whispers, John sat on the curb, shrugged off the paramedic\u2019s question, and mused that the accident was \u201cjust the universe playing bumper cars with reality.\u201d He later dried his books with a hairdryer and noted in his journal that \u201caccidents are life\u2019s way of reminding us that control is an illusion,\u201d showing a stoic, almost amused acceptance rather than fear or distress.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 28.922829151153564,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03553299492385787,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18181818181818182,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.922829151153564
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The fire was caused by a faulty wire in one of John\u2019s antique clocks.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 18.81728982925415,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.9090909090909091,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.8125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 18.81728982925415
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John reacted with calm indifference. He stood with his hands in his pockets, watched the smoke, and remarked that the house had \u201cdecided to retire early.\u201d He was not distraught; instead he observed the ruins, made a dry joke, and pulled out a notebook to jot down, \u201cNote to self: Fire sales are literal.\u201d",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 32.815929889678955,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.12350597609561753,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18032786885245902,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7419354838709677,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 32.815929889678955
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The archive closed because the university decided to digitize everything and shut down the physical stacks. John was packing a 17th\u2011century edition of Montaigne\u2019s *Essays* as the last book.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 27.422595024108887,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7076923076923077,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.375,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.90625,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 27.422595024108887
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "At 21, John was let go from the Evergreen Public Library. The head librarian, Mrs.\u202fHargrove, called him into her office and handed him a folded letter announcing that his contract would not be renewed next semester because of budget cuts\u2014three positions were being eliminated, including his. He left the office, returned to his circulation desk to finish the day, and later applied for other jobs, eventually taking a position as a cataloguing assistant at a private rare\u2011book dealer.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 43.636481285095215,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.019639934533551555,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.13924050632911392,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.3333333333333333,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 43.636481285095215
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "John applied for three positions:  \n1. Night security at a self\u2011storage facility.  \n2. Part\u2011time barista at a shop that served only oat\u2011milk lattes.  \n3. Cataloguing assistant at a private rare\u2011book dealer (who required a handwritten essay on why dust is noble).",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 28.277933835983276,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7619047619047619,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.40816326530612246,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0512820512820513,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.277933835983276
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "He received a notice of a class\u2011action settlement for a chemical plant leak that had contaminated the river supplying Evergreen\u2019s water, offering modest compensation for potential long\u2011term health effects.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 24.595762968063354,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08947368421052632,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2894736842105263,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2083333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 24.595762968063354
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "John\u2019s sister Mara was diagnosed with an aggressive form of leukemia. She passed away three weeks later, quietly, while John was reading to her.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 30.041149616241455,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5015873015873016,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 30.041149616241455
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "John stayed with Mara in the hospital, reading to her, making instant coffee, and simply sitting beside her while the IV dripped. He offered calm, philosophical remarks and remained present even as she died quietly after three weeks. After her passing, he finished the paragraph he was reading, then sat with her for a long time listening to the machines wind down. He returned to Evergreen, added her hospital bracelet to his small wooden memento box (labeling it \u201cAct\u202fIII\u201d), and carried on with his routine, carrying the weight like an umbrella.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 101.6433174610138,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03768506056527591,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.26506024096385544,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 101.6433174610138
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "John was diagnosed with Stage\u202fIV pancreatic cancer. He listened quietly, nodded once, asked only one question about how long the coffee would taste like coffee, then accepted the news calmly, kept his job, and simply adjusted his routine.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 28.74609684944153,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2374429223744292,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2580645161290323,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.74609684944153
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "John\u2019s final requests were:\n\n1. A fresh pot of truly terrible coffee.  \n2. That someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 37.97168850898743,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8525641025641025,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6296296296296297,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.25,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 37.97168850898743
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "ERROR: The following component failed to run:\nComponent name: 'llm'\nComponent type: 'OllamaGenerator'\nError: timed out",
      "retrieved_docs": [],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 0.0,
      "retrieval_count": 0,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "Pipeline",
          "metric_name": "pipeline_error",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "error": "The following component failed to run:\nComponent name: 'llm'\nComponent type: 'OllamaGenerator'\nError: timed out"
          }
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "John\u2019s view of tragedy was consistently calm, accepting, and framed as an absurd, inevitable part of life. He saw accidents and loss as reminders that control is an illusion and that \u201cthe wheel keeps turning.\u201d Rather than rage or dramatize, he treated tragedy as the universe\u2019s way of editing the plot\u2014\u201cwhy fight the farce?\u201d or \u201cwhy rage against the red pen?\u201d He accepted impermanence (\u201cbeauty doesn\u2019t always come with permanence\u201d), kept a stoic, almost humorous stance (\u201cthe absurd has an ending or just another act\u201d), and simply carried on, viewing each tragedy as a footnote in an endless, ridiculous manuscript.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 81.18023443222046,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.021251475796930343,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.19148936170212766,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.6666666666666666,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.5,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 81.18023443222046
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.75,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "**John Doe\u2019s known employment history (as stated in the provided texts)**  \n\n| Period / Age | Job Title | Employer / Context |\n|--------------|-----------|--------------------|\n| 21\u202fyrs old | Junior assistant | Evergreen Public Library |\n| 21\u202fyrs old (after dismissal) | Night\u2011security guard | Self\u2011storage facility |\n| 21\u202fyrs old (after dismissal) | Part\u2011time barista | Oat\u2011milk\u2011lattes specialty shop |\n| 21\u202fyrs old (after dismissal) | Cataloguing assistant | Private rare\u2011book dealer (hand\u2011written essay requirement) |\n| 28\u202fyrs old (and thereafter) | Archivist / cataloguer | University archive (cataloguing fragile first editions) |\n\nThese are the only jobs explicitly mentioned in the supplied context.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 85.63844060897827,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.06615214994487321,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.5,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 4,
            "matched_files": [
              "the-library-of-small-disappointments.txt",
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_10",
          "value": 0.5,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 85.63844060897827
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 1.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 15,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.0,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 0.0,
    "AnswerEvaluator_exact_match_count": 14,
    "AnswerEvaluator_sequence_similarity_mean": 0.326734708080215,
    "AnswerEvaluator_sequence_similarity_min": 0.019639934533551555,
    "AnswerEvaluator_sequence_similarity_max": 0.9090909090909091,
    "AnswerEvaluator_sequence_similarity_count": 14,
    "AnswerEvaluator_word_overlap_mean": 0.3312753038545379,
    "AnswerEvaluator_word_overlap_min": 0.1,
    "AnswerEvaluator_word_overlap_max": 0.8125,
    "AnswerEvaluator_word_overlap_count": 14,
    "AnswerEvaluator_answer_length_ratio_mean": 1.4511950846772275,
    "AnswerEvaluator_answer_length_ratio_min": 0.75,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 14,
    "AnswerEvaluator_keyword_match_mean": 1.0,
    "AnswerEvaluator_keyword_match_min": 1.0,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 14,
    "RetrievalEvaluator_context_match_mean": 0.9404761904761905,
    "RetrievalEvaluator_context_match_min": 0.5,
    "RetrievalEvaluator_context_match_max": 1.0,
    "RetrievalEvaluator_context_match_count": 14,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7738095238095237,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.3333333333333333,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 1.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 14,
    "RetrievalEvaluator_recall_at_3_mean": 0.9226190476190476,
    "RetrievalEvaluator_recall_at_3_min": 0.25,
    "RetrievalEvaluator_recall_at_3_max": 1.0,
    "RetrievalEvaluator_recall_at_3_count": 14,
    "RetrievalEvaluator_recall_at_5_mean": 0.9404761904761905,
    "RetrievalEvaluator_recall_at_5_min": 0.5,
    "RetrievalEvaluator_recall_at_5_max": 1.0,
    "RetrievalEvaluator_recall_at_5_count": 14,
    "RetrievalEvaluator_recall_at_10_mean": 0.9404761904761905,
    "RetrievalEvaluator_recall_at_10_min": 0.5,
    "RetrievalEvaluator_recall_at_10_max": 1.0,
    "RetrievalEvaluator_recall_at_10_count": 14,
    "RetrievalEvaluator_precision_at_10_mean": 0.2857142857142857,
    "RetrievalEvaluator_precision_at_10_min": 0.25,
    "RetrievalEvaluator_precision_at_10_max": 0.5,
    "RetrievalEvaluator_precision_at_10_count": 14,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 14,
    "PerformanceEvaluator_retrieval_efficiency_mean": 0.3392857142857143,
    "PerformanceEvaluator_retrieval_efficiency_min": 0.25,
    "PerformanceEvaluator_retrieval_efficiency_max": 1.0,
    "PerformanceEvaluator_retrieval_efficiency_count": 14,
    "PerformanceEvaluator_answer_provided_mean": 1.0,
    "PerformanceEvaluator_answer_provided_min": 1.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 14,
    "Pipeline_pipeline_error_mean": 0.0,
    "Pipeline_pipeline_error_min": 0.0,
    "Pipeline_pipeline_error_max": 0.0,
    "Pipeline_pipeline_error_count": 1,
    "per_difficulty": {
      "easy": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.37983105848767523,
        "AnswerEvaluator_word_overlap_mean": 0.461186229382432,
        "AnswerEvaluator_answer_length_ratio_mean": 1.3717171717171717,
        "AnswerEvaluator_keyword_match_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7777777777777778,
        "RetrievalEvaluator_recall_at_3_mean": 1.0,
        "RetrievalEvaluator_recall_at_5_mean": 1.0,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_10_mean": 0.25,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_efficiency_mean": 0.25,
        "PerformanceEvaluator_answer_provided_mean": 1.0,
        "Pipeline_pipeline_error_mean": 0.0
      },
      "medium": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.2825391978140067,
        "AnswerEvaluator_word_overlap_mean": 0.27665884470241675,
        "AnswerEvaluator_answer_length_ratio_mean": 1.3107638888888888,
        "AnswerEvaluator_keyword_match_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.9166666666666666,
        "RetrievalEvaluator_recall_at_3_mean": 1.0,
        "RetrievalEvaluator_recall_at_5_mean": 1.0,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_10_mean": 0.25,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_efficiency_mean": 0.25,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "hard": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.34791151015518873,
        "AnswerEvaluator_word_overlap_mean": 0.318868499520347,
        "AnswerEvaluator_answer_length_ratio_mean": 1.6673992673992672,
        "AnswerEvaluator_keyword_match_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 0.8333333333333333,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.6,
        "RetrievalEvaluator_recall_at_3_mean": 0.7833333333333333,
        "RetrievalEvaluator_recall_at_5_mean": 0.8333333333333333,
        "RetrievalEvaluator_recall_at_10_mean": 0.8333333333333333,
        "RetrievalEvaluator_precision_at_10_mean": 0.35,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_efficiency_mean": 0.5,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      }
    },
    "success_rate": 0.9333333333333333,
    "avg_response_time": 42.730444414275034
  }
}