{
  "batch_id": "batch_20260202_105638",
  "timestamp": "2026-02-02T10:56:38.990257",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The bus skidded on wet pavement and crashed into a fallen tree. The impact caused screams, shattered glass and burnt\u2011rubber fumes. John survived without injury, but his best friend Tommy broke an arm and the bus driver was hospitalized.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 27.927273273468018,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.21076233183856502,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4318181818181818,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.1818181818181819,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 27.927273273468018
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "John stayed calm and detached, sitting on the curb and replying with a philosophical shrug (\u201cScary? It\u2019s just the universe playing bumper cars with reality\u201d). He later noted in his journal that accidents remind us control is an illusion. In contrast, the community reacted with anxiety\u2014parents hovered like helicopters, counselors offered group hugs, and others replayed the horror in whispers.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 23.849408388137817,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13526570048309178,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.23880597014925373,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7647058823529411,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 23.849408388137817
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The fire was caused by a faulty wire in one of John\u2019s antique clocks.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 18.853084564208984,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.9090909090909091,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.8125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 18.853084564208984
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John reacted with calm indifference. He stood with his hands in his pockets, watched the smoke, and remarked evenly that the house had \u201cdecided to retire early.\u201d He was not distraught; he simply observed the ruins, made a light\u2011hearted comment, and jotted a note in his notebook: \u201cNote to self: Fire sales are literal.\u201d",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 33.173015117645264,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2553191489361702,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18333333333333332,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7419354838709677,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-library-of-small-disappointments.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 33.173015117645264
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The archive was shut because the university decided to digitize all its holdings and close the physical stacks. John was packing a 17th\u2011century edition of Montaigne\u2019s *Essays* as the last book.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 30.457676887512207,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.6954314720812182,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.36585365853658536,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.96875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-last-book.txt"
            ],
            "expected_docs": [
              "the-last-book.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 30.457676887512207
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "John was told by the head librarian that his contract would not be renewed next semester and that three positions were being eliminated\u2014including his. He left the office, returned to his desk to finish the day, and later applied for other jobs.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 39.917880058288574,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.026954177897574125,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.13725490196078433,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9090909090909092,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "expected_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": [
              "the-wedding-that-wasnt.txt",
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 39.917880058288574
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "John applied for three positions:  \n1. Night security at a self\u2011storage facility.  \n2. Part\u2011time barista at a shop that served only oat\u2011milk lattes.  \n3. Cataloguing assistant at a private rare\u2011book dealer (who required a handwritten essay on why dust is noble).",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 27.036471128463745,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7619047619047619,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.40816326530612246,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0512820512820513,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "expected_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": [
              "the-library-of-small-disappointments.txt",
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 27.036471128463745
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "He received a notice of a class\u2011action settlement for a chemical plant leak that had contaminated Evergreen\u2019s river water, offering modest compensation for potential long\u2011term health effects.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 24.66961646080017,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7377049180327869,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3055555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-ordinary-tuesday.txt"
            ],
            "expected_docs": [
              "the-ordinary-tuesday.txt"
            ],
            "retrieved_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 24.66961646080017
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "John\u2019s sister Mara died three weeks after her diagnosis. She passed away quietly in the hospital while John was reading aloud from a philosophy book, and he stayed with her until the machines wound down into silence.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 23.398710012435913,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08785529715762273,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15384615384615385,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.15625,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-last-book.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 23.398710012435913
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "John stayed with Mara in the hospital, reading to her, making instant coffee, and simply sitting beside her while the IV dripped. He remained calm and composed, earning the nickname \u201cthe calm one\u201d from nurses. After she died quietly, he returned to Evergreen, added her hospital bracelet to a small wooden box (labeling it \u201cAct\u202fIII\u201d), and carried that memory with him as he resumed his ordinary routine, continuing to live with the same steady, unflustered attitude.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 41.23239612579346,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.045317220543806644,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 41.23239612579346
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "John Doe was diagnosed with Stage\u202fIV pancreatic cancer at age\u202f32. He listened quietly, nodded once, asked a single question about how long the coffee would taste like coffee, and then left the office carrying only a pamphlet titled *Living with Advanced Illness*, continuing his routine without quitting his job.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 30.559028387069702,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.12109375,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.22950819672131148,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6451612903225807,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-uninvited-guest.txt"
            ],
            "expected_docs": [
              "the-uninvited-guest.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 30.559028387069702
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "John\u2019s final requests were:\n\n1. A fresh pot of truly terrible coffee.  \n2. That someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 30.08917260169983,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8525641025641025,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6296296296296297,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-uninvited-guest.txt"
            ],
            "expected_docs": [
              "the-uninvited-guest.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 30.08917260169983
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "John\u2019s wedding never took place. Lila called the night before and told him she couldn\u2019t go, citing her own panic rather than any fault of his. When the ceremony began, guests drifted away, and John waited in the empty church for a last\u2011minute change of heart that never came. He left the church, met the street vendor, and walked home, accepting that the event had been called off.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 32.41462850570679,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.12710280373831775,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1388888888888889,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-wedding-that-wasnt.txt"
            ],
            "expected_docs": [
              "the-wedding-that-wasnt.txt"
            ],
            "retrieved_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 32.41462850570679
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "John\u2019s view of tragedy is that it is an inevitable, impermanent part of life that should be met with calm acceptance rather than anger or despair. He repeatedly frames it as an \u201cediting\u201d of the plot\u2014\u201ctragedy is just the universe\u2019s way of editing the plot, why rage against the red pen?\u201d\u2014and sees it as a reminder that control is an illusion. In each story he does not cry or pace; he simply observes, notes the absurdity (\u201clife\u2019s tragedies were mere footnotes in an endless, ridiculous manuscript\u201d), and continues on. His philosophy is to treat tragedy as a footnote or a quiet edit, remain stoic, and keep moving forward.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 88.36045455932617,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03747072599531616,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5833333333333333,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.6666666666666666,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-uninvited-guest.txt",
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 88.36045455932617
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.75,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "**John Doe\u2019s known employment history (from the provided stories):**\n\n| Period | Job title | Employer / setting | Notes |\n|--------|-----------|--------------------|-------|\n| Early 20s | Junior assistant | Evergreen Public Library | \u201cnewly employed as a junior assistant\u201d |\n| Early\u2011mid 20s | Cataloguer of rare books | University archive | \u201cpart\u2011time job cataloguing rare books at the university archive\u201d |\n| After contract termination | Cataloguing assistant | Private rare\u2011book dealer | \u201ccataloguing assistant at a private rare\u2011book dealer\u201d |\n| Temporary/side work | Night security guard | Self\u2011storage facility | \u201cnight security at a self\u2011storage facility\u201d |\n| Temporary/side work | Part\u2011time barista | Oat\u2011milk\u2011only coffee shop | \u201cpart\u2011time barista at a shop that only served oat\u2011milk lattes\u201d |\n| (Same as the university archive role) | Archivist / cataloguer | Archive (as referenced in \u201cThe Uninvited Guest\u201d) | He \u201cdid not quit his job at the archive\u201d |\n\nNo other jobs are mentioned in the provided context.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 85.44395995140076,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02633744855967078,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.12121212121212122,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.5,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-library-of-small-disappointments.txt"
            ],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-library-of-small-disappointments.txt",
              "the-last-book.txt",
              "the-ordinary-tuesday.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-wedding-that-wasnt.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 85.44395995140076
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 1.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 15,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.0,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 0.0,
    "AnswerEvaluator_exact_match_count": 15,
    "AnswerEvaluator_sequence_similarity_mean": 0.3353449845882609,
    "AnswerEvaluator_sequence_similarity_min": 0.02633744855967078,
    "AnswerEvaluator_sequence_similarity_max": 0.9090909090909091,
    "AnswerEvaluator_sequence_similarity_count": 15,
    "AnswerEvaluator_word_overlap_mean": 0.3029246571305281,
    "AnswerEvaluator_word_overlap_min": 0.12121212121212122,
    "AnswerEvaluator_word_overlap_max": 0.8125,
    "AnswerEvaluator_word_overlap_count": 15,
    "AnswerEvaluator_answer_length_ratio_mean": 1.5175360945190166,
    "AnswerEvaluator_answer_length_ratio_min": 0.9333333333333333,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 15,
    "AnswerEvaluator_keyword_match_mean": 1.0,
    "AnswerEvaluator_keyword_match_min": 1.0,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 15,
    "RetrievalEvaluator_mean_average_precision_mean": 0.7944444444444444,
    "RetrievalEvaluator_mean_average_precision_min": 0.3333333333333333,
    "RetrievalEvaluator_mean_average_precision_max": 1.0,
    "RetrievalEvaluator_mean_average_precision_count": 15,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7888888888888889,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.3333333333333333,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 1.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 15,
    "RetrievalEvaluator_document_recall_mean": 1.0,
    "RetrievalEvaluator_document_recall_min": 1.0,
    "RetrievalEvaluator_document_recall_max": 1.0,
    "RetrievalEvaluator_document_recall_count": 15,
    "RetrievalEvaluator_context_match_mean": 0.9444444444444444,
    "RetrievalEvaluator_context_match_min": 0.5,
    "RetrievalEvaluator_context_match_max": 1.0,
    "RetrievalEvaluator_context_match_count": 15,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 15,
    "PerformanceEvaluator_retrieval_efficiency_mean": 0.3333333333333333,
    "PerformanceEvaluator_retrieval_efficiency_min": 0.25,
    "PerformanceEvaluator_retrieval_efficiency_max": 1.0,
    "PerformanceEvaluator_retrieval_efficiency_count": 15,
    "PerformanceEvaluator_answer_provided_mean": 1.0,
    "PerformanceEvaluator_answer_provided_min": 1.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 15,
    "per_difficulty": {
      "easy": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.3184775556413415,
        "AnswerEvaluator_word_overlap_mean": 0.38011549316696375,
        "AnswerEvaluator_answer_length_ratio_mean": 1.506060606060606,
        "AnswerEvaluator_keyword_match_mean": 1.0,
        "RetrievalEvaluator_mean_average_precision_mean": 0.8333333333333334,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8333333333333334,
        "RetrievalEvaluator_document_recall_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_efficiency_mean": 0.25,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "medium": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.33877838111514835,
        "AnswerEvaluator_word_overlap_mean": 0.2461504780236989,
        "AnswerEvaluator_answer_length_ratio_mean": 1.4003004427577483,
        "AnswerEvaluator_keyword_match_mean": 1.0,
        "RetrievalEvaluator_mean_average_precision_mean": 0.9166666666666666,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.9166666666666666,
        "RetrievalEvaluator_document_recall_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_efficiency_mean": 0.25,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "hard": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.34471885191353163,
        "AnswerEvaluator_word_overlap_mean": 0.3093010032295747,
        "AnswerEvaluator_answer_length_ratio_mean": 1.6673992673992672,
        "AnswerEvaluator_keyword_match_mean": 1.0,
        "RetrievalEvaluator_mean_average_precision_mean": 0.6166666666666666,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.6,
        "RetrievalEvaluator_document_recall_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 0.8333333333333333,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_efficiency_mean": 0.5,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      }
    },
    "success_rate": 1.0,
    "avg_response_time": 37.15885173479716
  }
}