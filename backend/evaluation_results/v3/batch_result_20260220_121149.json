{
  "batch_id": "batch_20260220_115112",
  "timestamp": "2026-02-20T11:51:12.581124",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John emerged unscathed, but his best friend, Tommy, suffered a broken arm, and the bus driver was hospitalized.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 20.616703271865845,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7837150127226463,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6470588235294118,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9393939393939394,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.85,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9814654955927329,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 20.616703271865845
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The provided context mentions that the fire was caused by \"a faulty wire in one of his beloved clocks.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 12.87785530090332,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7457627118644068,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.5714285714285714,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2666666666666666,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8571428571428571,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9318043125372582,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.87785530090332
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "According to the context, when John Doe was twenty-one and working as a junior assistant in the Evergreen Public Library, the head librarian, Mrs. Hargrove, called him into her office on a gray Thursday afternoon. She informed him that his contract would not be renewed due to budget cuts, and three positions were being eliminated, including his.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 14.043531656265259,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2742616033755274,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.21666666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7915639343460903,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.043531656265259
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "The provided context does not contain information about what happened at John's wedding. It mentions that Lila called him the night before and said she couldn't go through with it, but it doesn't describe the actual event or its outcome.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 13.905498266220093,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02557544757033248,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.12727272727272726,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.3793103448275863,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.375,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6466103211704793,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.905498266220093
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How old was John when the school bus accident happened?",
      "expected_answer": "John was fifteen when the school bus accident occurred.",
      "generated_answer": "The provided context mentions that John was fifteen years old at the time of the school bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 12.184143543243408,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5641025641025641,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3157894736842105,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9137590092905106,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.184143543243408
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What injury did Tommy suffer in the bus accident?",
      "expected_answer": "Tommy suffered a broken arm in the bus accident.",
      "generated_answer": "The context mentions... \n\nTommy suffered a broken arm in the school bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 12.146687030792236,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7441860465116279,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4444444444444444,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8859268051373694,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.146687030792236
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John call the club he started after the bus accident?",
      "expected_answer": "John started a 'Bus Philosophy Club' after the bus accident.",
      "generated_answer": "The provided context does not contain information about John starting a club after the bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 12.758600234985352,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5125,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3157894736842105,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.42857142857142855,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7536271151375509,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.758600234985352
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John use to dry his books after the bus accident?",
      "expected_answer": "John dried his books with a hairdryer after the bus accident.",
      "generated_answer": "According to the context, John used a hairdryer to dry his books.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 12.658064126968384,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.4444444444444444,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.29411764705882354,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0909090909090908,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.42857142857142855,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8012643117310982,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.658064126968384
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What profession did John have when his house burned down?",
      "expected_answer": "John worked as a librarian when his house burned down.",
      "generated_answer": "The provided context does not contain information about what profession John had at the time of his house burning down. However, it is mentioned that he worked as a librarian in \"The Ordinary Tuesday\" section, but this is not directly related to the event of his house burning down.\n\nHowever, based on the context from \"The Calm Storm of John Doe\", it can be inferred that John was a librarian at some point in his life.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 14.313178062438965,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13924050632911392,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14545454545454545,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8385755015715655,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.313178062438965
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John collect that caused the house fire?",
      "expected_answer": "John collected antique clocks, and a faulty wire in one caused the house fire.",
      "generated_answer": "The provided context mentions that John collected antique clocks. It is stated that \"one of his beloved clocks\" had a faulty wire, which caused the house to catch fire.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 13.108641624450684,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5853658536585366,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.28125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8888888888888888,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9078068274902311,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.108641624450684
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do with the pigeons after the house fire?",
      "expected_answer": "John fed crumbs from a stale sandwich to pigeons after the house fire.",
      "generated_answer": "John sat on a bench in the local park, feeding pigeons crumbs from a stale sandwich he'd forgotten in his car.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 12.102893590927124,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5222222222222223,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6153846153846154,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7942101311472592,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.102893590927124
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What type of clocks did John start collecting after the fire?",
      "expected_answer": "John started collecting digital clocks after the fire.",
      "generated_answer": "The provided context does not explicitly state what type of clocks John started collecting after the fire. However, it mentions that he started a new clock collection, \"this time digital ones that couldn't spark.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 13.18112564086914,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2546816479400749,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25806451612903225,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8511590151486046,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.18112564086914
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What book was John packing last in the archive?",
      "expected_answer": "John was packing a 17th-century edition of Montaigne\u2019s Essays last.",
      "generated_answer": "The context mentions that John spent the last three weeks of the archive's open life packing volumes into acid-free boxes. On the final day, he was alone in the dim basement room, sealing the last carton. Inside it lay a 17th-century edition of Montaigne's *Essays*. \n\nTherefore, the book John packed last in the archive was a 17th-century edition of Montaigne's *Essays*.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 14.405813455581665,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.22779043280182232,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15555555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7816370621833681,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.405813455581665
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What phrase was in the last book John packed?",
      "expected_answer": "The phrase 'Que s\u00e7ay-je?' (What do I know?) was in the last book.",
      "generated_answer": "The provided context mentions that inside the last carton John packed lay a 17th-century edition of Montaigne's *Essays*. The specific page he opened to before sealing the box is not mentioned, but it is noted that he had read this page \"a hundred times.\" However, the phrase in question is not explicitly stated as being on this particular page.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 14.153834581375122,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.06326034063260341,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.07142857142857142,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2857142857142857,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6654491316616822,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.153834581375122
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How long did John spend packing volumes in the archive?",
      "expected_answer": "John spent the last three weeks packing volumes in the archive.",
      "generated_answer": "The context mentions that John spent \"the last three weeks\" of the archive's open life packing volumes into acid-free boxes. \n\nNote: The exact number of days is not specified, only that it was for three weeks.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 13.329955577850342,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.375,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.20588235294117646,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7101441448468255,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.329955577850342
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John keep from the archive as a private act?",
      "expected_answer": "John kept the call number of Montaigne\u2019s Essays as a private act.",
      "generated_answer": "The provided context does not contain information to answer this question about \"The Calm Storm of John Doe\" or \"The Library of Small Disappointments\". However, it can be answered based on \"The Last Book\".\n\nJohn kept the call number of Montaigne's *Essays* as a small, private act of defiance against total digitization.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 13.103021621704102,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2961038961038961,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.75,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7578732804512356,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.103021621704102
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Who was the head librarian at Evergreen Public Library?",
      "expected_answer": "Mrs. Hargrove was the head librarian at Evergreen Public Library.",
      "generated_answer": "Mrs. Hargrove was the head librarian at Evergreen Public Library.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 12.898709058761597,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 1.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 1.0000000000000013,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.898709058761597
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How many positions were eliminated at the library?",
      "expected_answer": "Three positions were eliminated at the library.",
      "generated_answer": "The head librarian, Mrs. Hargrove\u2014a woman who treated overdue fines with the gravity of international law\u2014called him into her office on a gray Thursday afternoon.\n\nShe slid a folded letter across the desk. \u201cI\u2019m afraid we won\u2019t be renewing your contract next semester,\u201d she said, voice flat. \u201cBudget cuts. Last in, first out.\u201d\n\nJohn read the letter once, then refolded it with the same precision he used for damaged book spines. He looked up. \u201cHow many positions are being eliminated?\u201d\n\n\u201cThree. Yours included.\u201d",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 14.818557977676392,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.025134649910233394,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.02531645569620253,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6878855754006898,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.818557977676392
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was the rare-book dealer's unusual requirement?",
      "expected_answer": "The rare-book dealer required a handwritten essay on why dust is noble.",
      "generated_answer": "The rare-book dealer required applicants to submit a handwritten essay on why dust is noble.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 12.876172542572021,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8711656441717791,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.8,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.961191191895858,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.876172542572021
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John note about dust in his new job?",
      "expected_answer": "John decided that the dust was rather noble in his new job.",
      "generated_answer": "The provided context does not contain information on what John noted about dust in his new job as cataloguing assistant at the private rare-book dealer. However, it is mentioned that he decided \"the dust, he decided, really was rather noble.\"",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 12.67856740951538,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13953488372093023,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8571428571428571,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.81647182429121,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.67856740951538
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "The provided context mentions that when the school bus accident occurred, chaos ensued with screams, shattered glass, and the acrid smell of burnt rubber. However, John emerged unscathed but his best friend Tommy suffered a broken arm, and the bus driver was hospitalized.\n\nIn contrast to others who were shaken by the incident, John sat on the curb, observing the scene calmly. When approached by a paramedic, he replied with a shrug: \"Scary? It's just the universe playing bumper cars with reality.\" He also noted in his journal that accidents are life's way of reminding us that control is an illusion.\n\nJohn's reaction was described as turning the tragedy into quiet absurdity, starting a \"Bus Philosophy Club\" at school where members discussed existentialism over vending machine snacks.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 15.951443195343018,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.15701668302257116,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1271186440677966,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.46153846153846156,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7454364085029233,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.951443195343018
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John reacted calmly. He stood outside his burning house, watching the flames with a sense of detachment. When asked by a neighbor what he would do, he replied that \"the house has decided to retire early\" and joked that he had thought about it himself on Mondays. He then pulled out a notebook and jotted down: \"Note to self: Fire sales are literal.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 13.547676801681519,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.09140767824497258,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.17142857142857143,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8075346954487629,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.547676801681519
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The university archive closed because the university decided to digitize everything and close the physical stacks. This decision allowed for \"preservation\" of the rare books in climate-controlled storage, with access available by appointment only through high-resolution scans viewed on screens.\n\nJohn was packing a 17th-century edition of Montaigne's *Essays* when he finished sealing the last carton.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 13.851894855499268,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.44776119402985076,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.31666666666666665,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8140841589585082,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.851894855499268
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "The provided context mentions that John received a class-action settlement notice related to a chemical plant leak into the river that supplied Evergreen's water. The notice offered modest compensation for \"potential long-term health effects.\"",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 13.628530979156494,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.41626794258373206,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3684210526315789,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4166666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.75,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9019458732173837,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.628530979156494
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "According to the provided context in \"The Quiet Clockwork of John Doe,\" John's sister Mara was diagnosed with an aggressive form of leukemia and had only months to live. However, it is not mentioned what ultimately happened to her. The story jumps forward to John visiting her at the hospital, where she is described as looking smaller than he remembered, swallowed by white sheets and tubes, but still holding a sharp amusement in her eyes.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 14.003720760345459,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.10784313725490197,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18666666666666668,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.45,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7863440370244574,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.003720760345459
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "The provided context mentions that John Doe was thirty-two when the diagnosis came, delivered by an oncologist. The diagnosis was pancreatic cancer, Stage IV.\n\nJohn listened to the doctor's measured sentences, nodded once, and asked only one question: \"How long until the coffee stops tasting like coffee?\" He then left the office carrying a pamphlet titled *Living with Advanced Illness*.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 14.270045518875122,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.200339558573854,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.21212121212121213,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.935483870967742,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8576567366085805,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.270045518875122
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do the day after losing his library job?",
      "expected_answer": "The day after losing his job, John sat at his desk and wrote notes, then planned to apply for three more jobs, one of which he expected to be ridiculous.",
      "generated_answer": "The provided context does not contain information about what John did the day after losing his library job. It mentions that he left the office, returned to the circulation desk, and spent the rest of his shift reshelving returns with the same quiet rhythm as always. However, it does not specify his activities for the next day.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 13.809292316436768,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.17842323651452283,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1746031746031746,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.25,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.732150819809174,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.809292316436768
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did Mara die and what was John's immediate action?",
      "expected_answer": "Mara died quietly while John was reading to her from a chapter on the indifference of the stars. He finished the paragraph before closing the book.",
      "generated_answer": "The provided context does not contain information about Mara's death or John's immediate action regarding it.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 12.366405248641968,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.203125,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.02631578947368421,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.6153846153846154,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.13333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6805561182073776,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.16666666666666666,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 6
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.366405248641968
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say about fear to Mara?",
      "expected_answer": "John said to Mara that fear is just the body\u2019s last honest opinion, but she would find out whether the absurd has an ending or just another act.",
      "generated_answer": "The provided context does not contain information about a person named Mara or a conversation between John and Mara regarding fear.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 12.728428602218628,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.14545454545454545,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0975609756097561,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.16666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.656118748191607,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.125,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 8
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.728428602218628
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do with Mara's hospital bracelet?",
      "expected_answer": "John added Mara's hospital bracelet to his mementos box labeled 'Act III' after her death.",
      "generated_answer": "He returned to the apartment above the laundromat, added Mara's hospital bracelet to the small wooden box where he kept mementos, labeled it \"Act III\", and closed the lid.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 12.706359148025513,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5593869731800766,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.21875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5454545454545454,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8362093104508524,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.706359148025513
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did the oncologist describe John's prognosis?",
      "expected_answer": "The oncologist described John's prognosis as statistics that hovered between 'slim' and 'theoretical' for stage IV pancreatic cancer.",
      "generated_answer": "The context mentions that the oncologist spoke in measured sentences, offering statistics that hovered somewhere between \"slim\" and \"theoretical.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 13.099168539047241,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5591397849462365,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0555555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.38461538461538464,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8112808587498033,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.099168539047241
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What adjustments did John make after his diagnosis?",
      "expected_answer": "John adjusted his routine by fifteen minutes each morning to accommodate the new fatigue and continued his routines with minor adjustments.",
      "generated_answer": "The provided context mentions that after his diagnosis, John made the following adjustment:\n\n\"He simply adjusted his routine by fifteen minutes each morning to accommodate the new fatigue that arrived like an uninvited houseguest who refused to leave the couch.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 12.88818097114563,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.45885286783042395,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.35,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9047619047619047,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6428571428571429,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8715572287879816,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.88818097114563
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What note did John leave in his cupboard?",
      "expected_answer": "John left a note saying: 'If you\u2019re reading this, the coffee is probably stale. Make a new pot anyway. Life is absurd enough without drinking yesterday\u2019s grounds.'",
      "generated_answer": "The provided context mentions that Elena found a single sheet of paper taped inside the cupboard above the coffee maker with John's neat handwriting. The note read:\n\n\"If you\u2019re reading this, the coffee is probably stale. Make a new pot anyway. Life is absurd enough without drinking yesterday\u2019s grounds.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 13.493041515350342,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.6038543897216274,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.425531914893617,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8148148148148149,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8421052631578947,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9050364251642744,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.493041515350342
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do after the wedding was called off?",
      "expected_answer": "John went to the church, waited twenty minutes for a miracle, then walked out, bought a pretzel, ate half, and fed the rest to pigeons.",
      "generated_answer": "According to the context, \"At ten minutes past the hour the ceremony was meant to begin, John stood on the steps watching guests trickle away in polite confusion. An aunt patted his arm. A cousin offered awkward condolences. John thanked them each with the same gentle nod.\"\n\nWhen the last car pulled away, he walked inside the empty church. The altar flowers were still fresh; the programs lay stacked on a side table. He sat in the front pew, placed the rose beside him, and waited another twenty minutes\u2014long enough for any last-minute miracle to arrive fashionably late.\n\nNone did.\n\nHe stood, brushed invisible lint from his sleeve, and walked out into bright noon sunlight. On the sidewalk he met the street vendor again, now selling pretzels.\"\n\nSo, after the wedding was called off, John:\n\n* Watched guests leave\n* Sat in the empty church for 20 minutes\n* Walked out of the church\n* Met the street vendor and bought a pretzel",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 16.208479166030884,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.07692307692307693,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08661417322834646,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4666666666666667,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7103017682573678,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 16.208479166030884
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say to the vendor about the rose?",
      "expected_answer": "John said the rose worked perfectly, as it reminded him that beauty doesn\u2019t always come with permanence.",
      "generated_answer": "\"It worked perfectly,\" John replied. \"It just reminded me that beauty doesn\u2019t always come with permanence.\"",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 12.97742223739624,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7867298578199052,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.391304347826087,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9411764705882353,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8461538461538461,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8616342898452551,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.97742223739624
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John keep from the archive and why?",
      "expected_answer": "John kept a slip of paper with the call number of Montaigne\u2019s Essays as a small, private act of defiance against total digitization.",
      "generated_answer": "The provided context does not contain information to answer this question about what John kept from the archive and why. However, it mentions that he handled each book as though saying goodbye to an old friend who had never quite learned his name, indicating a sentimental attachment to the books. \n\nIt is also mentioned that he left the building carrying nothing but his coat and a single slip of paper with the call number of Montaigne's *Essays*.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 14.208767414093018,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.22375215146299485,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1506849315068493,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.5972211382926211,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.208767414093018
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say about missing the books?",
      "expected_answer": "John said missing is just another way of remembering, and remembering is free.",
      "generated_answer": "John said, \"I already do\" (missing the books), but then added, \"But missing is just another way of remembering. And remembering is free.\"",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 13.093575716018677,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7162790697674418,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.36,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7692307692307692,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.863197097008968,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.093575716018677
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John use the settlement money for?",
      "expected_answer": "John used the settlement money to buy a better coffee maker.",
      "generated_answer": "The provided context does not contain information on what John used the settlement money for. It only mentions that he read the amount and set the letter on the counter, but there is no further mention of how he utilized the funds.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 13.039337635040283,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.24054982817869416,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.12195121951219512,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.663859283932035,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.039337635040283
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John think about fighting entropy?",
      "expected_answer": "John thought fighting entropy is like yelling at gravity\u2014undignified.",
      "generated_answer": "The provided context does not contain information about John's thoughts on fighting entropy. However, it mentions that he noted in his journal: \"Accidents are life's way of reminding us that control is an illusion. Why fight the farce?\" This suggests that John has a philosophical perspective on accepting and adapting to the inevitability of chaos and disorder (entropy) rather than trying to resist or fight it.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 13.85188627243042,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.024896265560165973,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.047619047619047616,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7452187166600478,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.14285714285714285,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 7
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.85188627243042
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John cope with the house fire and what metaphor did he use?",
      "expected_answer": "John coped by accepting it calmly, using the metaphor that life's a library where books burn but stories float around waiting for new shelves.",
      "generated_answer": "The provided context mentions that after the house fire, John stood calmly observing the ruins, then pulled out a small notebook and jotted down \"Note to self: Fire sales are literal.\" He also wandered to the local park, feeding pigeons crumbs from a stale sandwich. When asked how he coped so calmly, he'd shrug and say, \"Tragedy is just the universe's way of editing the plot. Why rage against the red pen?\"\n\nJohn used the metaphor that tragedy is like the universe's editing process, comparing it to a writer using a red pen to edit a manuscript.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 15.20052981376648,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04052098408104197,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10112359550561797,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.35294117647058826,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6753793260023377,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.20052981376648
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "The provided context does not contain information about what specific jobs John applied for before taking the rare-book job at a private dealer. However, it mentions that he applied for three more jobs in total:\n\n1. Night security at a self-storage facility.\n2. Part-time barista at a shop that only served oat-milk lattes.\n3. Cataloguing assistant at a private rare-book dealer who insisted applicants submit a handwritten essay on why dust is noble.\n\nJohn took the third job, which was cataloguing assistant at the private rare-book dealer.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 14.955702781677246,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.37244897959183676,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4057971014492754,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.84,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8167485956426632,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.16666666666666666,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.955702781677246
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "The provided context does not contain information on how John handled his sister's death after she passed away in the hospital. It only describes their interactions before her passing, including his calm and philosophical demeanor during her illness.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 13.656994819641113,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04932735426008968,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09836065573770492,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0857142857142856,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.05,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7278593989574758,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.656994819641113
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "John's last requests were for a fresh pot of truly terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 13.743391513824463,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8831168831168831,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.76,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9050589813693819,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.743391513824463
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "The provided context does not contain a comprehensive answer to this question, as there are multiple instances of John facing different tragedies in various stages of his life. However, based on the available information:\n\nThroughout his life, John seemed to view tragedy with a sense of detachment and absurdity. He often responded to catastrophic events with philosophical musings, comparing them to plot twists or bad comedy.\n\nIn \"The Absurd Adolescence of John Doe,\" he described accidents as life's way of reminding us that control is an illusion (p. 4). He also started a \"Bus Philosophy Club\" at school, where members discussed existentialism over vending machine snacks (p. 5).\n\nIn \"The Calm Storm of John Doe,\" when his home caught fire, he observed the ruins and said, \"It seems the house has decided to retire early. Can't blame it\u2014I've thought about it myself on Mondays\" (p. 7). He also noted in his journal: \"Fire sales are literal\" (p. 8).\n\nIn \"The Uninvited Guest,\" when diagnosed with pancreatic cancer, he asked how long until the coffee would stop tasting like coffee and later said that fear is just the body's last honest opinion (p. 14-15). He also compared his life to a play where everyone knows the ending.\n\nJohn's philosophy about tragedy can be summarized as: tragedies are inevitable, control is an illusion, and life is absurd.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 18.941731452941895,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1070528967254408,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09714285714285714,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.34782608695652173,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7456237978819267,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 18.941731452941895
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "John had several jobs throughout his life, including:\n\n1. Junior assistant at the Evergreen Public Library (at age 21)\n2. Cataloguing assistant at a private rare-book dealer\n3. Part-time barista at a shop that only served oat-milk lattes\n4. Night security at a self-storage facility\n5. Librarian (shelving books and observing the stories they whispered)\n\nLater in life, he owned his own small secondhand bookstore.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 14.596243381500244,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1040650406504065,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1791044776119403,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7774018169247598,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 4,
            "expected_count": 4,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt",
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.596243381500244
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.7333333333333334,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's reactions to the bus accident and house fire reflect his philosophy?",
      "expected_answer": "In both the bus accident and house fire, John reacted with philosophical detachment and calm acceptance, seeing them as the universe's absurd events rather than personal tragedies, starting a philosophy club after the accident and rebuilding deliberately after the fire.",
      "generated_answer": "The provided context does not contain a direct comparison between John's reactions to the bus accident and house fire. However, it mentions that in both incidents, John remained calm and observed the situation with a philosophical perspective.\n\nIn the bus accident, John said, \"Scary? It's just the universe playing bumper cars with reality.\" He also noted in his journal: \"Accidents are life's way of reminding us that control is an illusion. Why fight the farce?\"\n\nSimilarly, after the house fire, John observed, \"It seems the house has decided to retire early. Can't blame it\u2014I've thought about it myself on Mondays.\" He also jotted down: \"Note to self: Fire sales are literal.\"\n\nJohn's reactions in both situations reflect his philosophy of accepting and observing life's tragedies as part of an absurd and unpredictable universe, where control is an illusion.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 15.987101793289185,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.05291005291005291,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15789473684210525,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4827586206896552,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8563612249660963,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.987101793289185
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What common theme connects John's job loss at the library and the archive closure?",
      "expected_answer": "Both events involved budget cuts or modernization leading to job changes; John lost his junior assistant role to cuts and the archive closed for digitization, but he adapted by finding new roles in rare books and eventually owning a bookstore.",
      "generated_answer": "The provided context does not explicitly state a direct connection between John's job loss at the library and the archive closure. However, both events involve changes to institutions where John worked with books.\n\nIn \"The Library of Small Disappointments,\" John loses his job due to budget cuts, but he remains calm and matter-of-fact about it.\n\nIn \"The Last Book,\" the university decides to digitize everything and close the physical stacks, effectively ending John's work in the archive. \n\nBoth events seem to be handled by John with a sense of detachment or acceptance, as if he views them as part of life's natural flow rather than personal tragedies.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt"
      ],
      "response_time": 15.43049669265747,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03114571746384872,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1568627450980392,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.32,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8235197156184575,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.43049669265747
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's handling of his sister's death compare to his own cancer diagnosis?",
      "expected_answer": "John handled both with stoicism and calm; he finished reading to Mara and labeled her bracelet 'Act III,' and for his cancer, he adjusted routines minimally and saw it as a predictable plot.",
      "generated_answer": "The provided context does not directly compare John's handling of his sister's death and his own cancer diagnosis. However, it mentions that when his sister Mara was diagnosed with leukemia, he \"did not cry. He did not pace. He simply sat at his small kitchen table, listening to the laundromat's rhythmic thump-thump below, and thought: *Well. The wheel keeps turning.*\"\n\nIn contrast, when John received his own diagnosis of pancreatic cancer, he asked only one question: \"How long until the coffee stops tasting like coffee?\" He then adjusted his routine by fifteen minutes each morning to accommodate the new fatigue.\n\nThe context does not provide a direct comparison between the two situations.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 16.023725271224976,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03153153153153153,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10476190476190476,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2631578947368421,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6426683975136405,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 16.023725271224976
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What role did coffee play in John's life across events?",
      "expected_answer": "Coffee was a constant ritual; he made terrible coffee during his cancer, requested terrible coffee as a last wish, and used settlement money for a better coffee maker, symbolizing normalcy amid tragedy.",
      "generated_answer": "Across the provided context, coffee played various roles in John's life:\n\n*   In \"The Uninvited Guest,\" John asked how long until his coffee would stop tasting like coffee after receiving a pancreatic cancer diagnosis.\n*   He continued to brew terrible instant coffee in the staff break room at the archive despite his illness.\n*   When Elena visited him, he mentioned that the coffee was probably stale and suggested making a new pot anyway.\n\nIn \"The Ordinary Tuesday,\" John made coffee that was exactly the right shade of bitter on his 53rd birthday. He also received notice of a class-action settlement related to water contamination in the river near his home, which he compared to an invoice from the universe, stating that fighting it would be undignified.\n\nOverall, coffee served as a mundane aspect of John's daily routine and was often used as a metaphor for life's impermanence and unpredictability.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 15.111918210983276,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.023402340234023402,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.391304347826087,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8263998167187331,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.111918210983276
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's wedding cancellation and job losses show his resilience?",
      "expected_answer": "John treated the wedding cancellation like a plot twist, feeding pigeons afterward, and after job losses from budget cuts and archive closure, he applied for new roles and adapted without distress.",
      "generated_answer": "The provided context does not contain information to directly answer how John's wedding cancellation and job losses showed his resilience. However, it can be inferred from various scenes that John exhibits a calm and accepting demeanor in the face of disappointment and adversity.\n\nIn \"The Wedding That Wasn't,\" when Lila cancels their wedding, John is understanding and waits at the church for her to change her mind, showing that he values their relationship over his own desires. When she doesn't show up, he leaves with a rose, symbolizing the beauty of their love but also acknowledging its impermanence.\n\nIn \"The Library of Small Disappointments,\" when John loses his job due to budget cuts, he is described as being \"inconvenienced\" rather than upset. He continues working at the circulation desk and later applies for new jobs, demonstrating a proactive approach to dealing with disappointment.\n\nThese scenes suggest that John's resilience stems from his ability to accept and adapt to changing circumstances, often finding meaning or humor in the midst of adversity.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt"
      ],
      "response_time": 17.011382341384888,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.029874213836477988,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09558823529411764,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3181818181818182,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.753529597882551,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 17.011382341384888
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What environmental event linked to John's settlement, and how did it tie to his health?",
      "expected_answer": "The chemical plant leak into Evergreen's water supply led to the settlement for potential health effects, which subtly connected to his later cancer diagnosis, though he accepted both without fury.",
      "generated_answer": "The provided context mentions that there was a chemical plant upstream from Evergreen that leaked into the river supplying the town's water years earlier. This leak is linked to John's settlement, as he receives notice of a class-action settlement for \"potential long-term health effects\" due to the contamination.\n\nHowever, it does not fully explain how this event ties directly to his health or if he has any specific health issues related to the environmental incident.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 14.452024459838867,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08669656203288491,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15384615384615385,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4090909090909091,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.839237979214205,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.452024459838867
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's philosophy evolve from adolescence to death?",
      "expected_answer": "From seeing accidents as 'bumper cars with reality' in adolescence, to accepting fire as 'editing the plot,' to viewing his cancer as a 'predictable plot' with curiosity about the ending, John consistently embraced life's absurdity.",
      "generated_answer": "The provided context does not contain information on how John's philosophy evolved from adolescence to death in detail. However, it can be inferred that his views remained consistent with the absurdism and acceptance of life's unpredictability he exhibited as a teenager.\n\nAs a teenager (in \"The Absurd Adolescence of John Doe\"), John observed life's tragedies with a detached perspective, viewing them as mere plot twists in an absurd narrative. He noted that accidents remind us control is an illusion and suggested accepting this reality.\n\nIn his mid-twenties (\"The Quiet Clockwork of John Doe\") and later when diagnosed with pancreatic cancer (\"The Uninvited Guest\"), John continued to apply this philosophical outlook. He saw life's challenges as part of the universe's \"bad comedy\" and accepted them with a calm, stoic demeanor.\n\nThroughout these periods, his philosophy remained centered on the idea that life is inherently absurd and unpredictable, and that one should learn to accept and even find humor in its tragedies.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 16.603282928466797,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.025356576862123614,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.11023622047244094,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.36,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7889099688456165,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 16.603282928466797
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What mementos did John keep from family tragedies?",
      "expected_answer": "From his sister's death, John kept her hospital bracelet in his 'Act III' mementos box; tragedies like parents' accident and sister's death shaped his stoic philosophy.",
      "generated_answer": "The provided context does not contain information to answer this question about what specific mementos John kept from family tragedies. However, it mentions that he kept a hospital bracelet labeled \"Act III\" after his sister Mara's diagnosis and treatment for leukemia.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 13.466917991638184,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.032036613272311214,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14545454545454545,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.5769230769230769,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.35,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8099697299164768,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.466917991638184
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did books and libraries thread through John's career setbacks?",
      "expected_answer": "John's library job loss led to rare-book cataloguing, then university archive work ending in digitization, culminating in owning a bookstore; books were constant amid changes.",
      "generated_answer": "The provided context does not contain information about how books and libraries specifically threaded through John's career setbacks. However, it mentions that John worked as a librarian in the Evergreen Public Library and later catalogued rare books at the university archive.\n\nIt can be inferred from the text that John has a deep connection to books and libraries, but the specific ways in which they relate to his career setbacks are not explicitly stated.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 14.429828882217407,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.05669291338582677,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09333333333333334,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3157894736842105,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7640466175215321,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.429828882217407
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What ironies did John note in his life events?",
      "expected_answer": "Ironies like a clock causing a fire, digitization closing an archive of physical books, and a water pollution settlement possibly linking to his cancer, all accepted as life's absurdities.",
      "generated_answer": "The provided context does not explicitly mention \"ironies\" but notes several instances where John observes and comments on the absurdity or contradictions in life. Some examples include:\n\n*   Accidents being a reminder that control is an illusion (\"Accidents are life's way of reminding us that control is an illusion.\")\n*   The universe running on bad comedy (\"I've always suspected the universe runs on bad comedy.\")\n*   Fear being the body's last honest opinion (\"Fear is just the body's last honest opinion.\")\n*   Life making sense being a claim made by those who don't understand it (\"But look at it this way: you're about to find out whether the absurd has an ending or just another act. Either way, you win the argument with everyone who ever claimed life makes sense.\")\n*   Fighting entropy (chaos) being like yelling at gravity (\"Fighting entropy is like yelling at gravity. Undignified.\")",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 16.322545289993286,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.01841620626151013,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0546875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.15789473684210525,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.665905550201423,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 4,
            "expected_count": 4,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt",
              "the-last-book.txt",
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 16.322545289993286
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.7333333333333334,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's relationships with friends and family show his detachment?",
      "expected_answer": "John offered observations instead of emotions to friends during cancer, was stoic with Mara during her death, and calmly accepted Lila's wedding cancellation.",
      "generated_answer": "The provided context does not contain information about how John's relationships with friends and family showed his detachment before the events of \"The Absurd Adolescence of John Doe.\" However, it mentions that after the school bus accident, John's parents fretted when he got home, expecting tears or tantrums. Instead, John methodically dried his books with a hairdryer and noted in his journal: \"Accidents are life's way of reminding us that control is an illusion.\"\n\nIn contrast, during the events of \"The Quiet Clockwork of John Doe,\" John visits his sister Mara in the hospital after receiving news about her illness. He does not cry or pace but instead thinks: \"Well. The wheel keeps turning.\" This shows a calm and detached demeanor when faced with personal tragedy.\n\nIt is only in \"The Calm Storm of John Doe\" that we see John's relationships with friends and family more clearly. After his house catches fire, a neighbor rushes over to offer sympathy, but John responds calmly, observing the ruins as if they were a peculiar cloud formation. He then jots down: \"Note to self: Fire sales are literal.\" This shows that even in the face of loss, John remains detached and matter-of-fact.\n\nHowever, it is worth noting that these instances might not necessarily demonstrate detachment but rather a stoic acceptance or a philosophical outlook on life's challenges.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 18.515432834625244,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04060248853962017,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.06962025316455696,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6717864594314811,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 18.515432834625244
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What common items symbolized normalcy in John's tragedies?",
      "expected_answer": "Coffee symbolized normalcy: terrible coffee during cancer, a new maker from settlement, and pretzels/pigeons after wedding, all grounding him in routine.",
      "generated_answer": "The provided context does not contain information to answer this question about what common items symbolized normalcy in John's tragedies. However, it mentions that John used a hairdryer to dry his books after the school bus accident and that he had clocks at home which were damaged in the fire.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 13.811753273010254,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.022271714922048998,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.13333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7028740967746032,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.14285714285714285,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 7
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.3333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.811753273010254
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did Evergreen feature in John's life story?",
      "expected_answer": "Evergreen was the suburb of his adolescence, home to his library job, water pollution settlement, and where he owned his bookstore amid various tragedies.",
      "generated_answer": "The provided context does not contain a comprehensive answer to how Evergreen featured in John's life story. However, it mentions that:\n\n* In \"The Quiet Clockwork of John Doe,\" John lived in a one-room apartment above a laundromat in Evergreen.\n* In \"The Absurd Adolescence of John Doe,\" John navigated his teenage years in the bustling suburb of Evergreen during the late 1990s.\n* In \"The Uninvited Guest,\" there is no mention of Evergreen, but it is mentioned that John worked at a university archive and later had friends who visited him.\n* In \"The Calm Storm of John Doe,\" John lived in a modest home in the quiet town of Evergreen.\n* In \"The Ordinary Tuesday,\" John owned a small secondhand bookstore in Evergreen.\n\nEvergreen seems to be a recurring location throughout John's life, serving as both a backdrop for his personal growth and struggles.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 16.635812520980835,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.023833167825223437,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0970873786407767,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.716960768231435,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.3333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 16.635812520980835
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What role did reading play in John's coping mechanisms?",
      "expected_answer": "John read philosophy to Mara, continued reading during chemo, and packed Montaigne's Essays last, using books to process absurdity and mortality.",
      "generated_answer": "Reading played a significant role in John's coping mechanisms. He often read philosophical texts, such as Camus' *The Myth of Sisyphus*, to find meaning and perspective on life's challenges. After the school bus accident, he noted in his journal that accidents are life's way of reminding us that control is an illusion, suggesting that reading had helped him develop a philosophical outlook. Later, when facing his sister's illness and then his own diagnosis, John continued to read and find solace in the words of others, using them to frame his experiences and maintain a sense of calm acceptance.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt"
      ],
      "response_time": 15.069864273071289,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.042953020134228186,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08139534883720931,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.29411764705882354,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7455143816069232,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.069864273071289
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's possessions reflect his philosophy?",
      "expected_answer": "Clocks (causing fire), mementos box ('Act III'), philosophy books, and coffee rituals all embodied acceptance of time, memory, and life's indifference.",
      "generated_answer": "John's possessions reflected his philosophy by being reminders of impermanence and the absurdity of life. His collection of antique clocks, which were destroyed in the fire, symbolized his belief that time is the only constant, but even it can be affected by chaos (e.g., a faulty wire). The fact that he kept mementos, such as Mara's hospital bracelet, in a small wooden box labeled \"Act III,\" suggests that he views life as a play with its own narrative arc. His journal entries and notes to himself also reflect his philosophical outlook, often incorporating humor and absurdity to cope with adversity.\n\nThe provided context does not fully explain the significance of each specific item or how they all relate to John's philosophy.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 14.682433843612671,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04971751412429379,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.045871559633027525,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.1875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7730086431978795,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.5333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 14.682433843612671
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 60,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.016666666666666666,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 1.0,
    "AnswerEvaluator_exact_match_count": 60,
    "AnswerEvaluator_sequence_similarity_mean": 0.2819337320152343,
    "AnswerEvaluator_sequence_similarity_min": 0.01841620626151013,
    "AnswerEvaluator_sequence_similarity_max": 1.0,
    "AnswerEvaluator_sequence_similarity_count": 60,
    "AnswerEvaluator_word_overlap_mean": 0.23711281729795378,
    "AnswerEvaluator_word_overlap_min": 0.02531645569620253,
    "AnswerEvaluator_word_overlap_max": 1.0,
    "AnswerEvaluator_word_overlap_count": 60,
    "AnswerEvaluator_answer_length_ratio_mean": 1.7553478125213606,
    "AnswerEvaluator_answer_length_ratio_min": 0.6153846153846154,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 60,
    "AnswerEvaluator_keyword_match_mean": 0.5447386516891098,
    "AnswerEvaluator_keyword_match_min": 0.05,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 60,
    "AnswerEvaluator_semantic_similarity_mean": 0.7892422261428197,
    "AnswerEvaluator_semantic_similarity_min": 0.5972211382926211,
    "AnswerEvaluator_semantic_similarity_max": 1.0000000000000013,
    "AnswerEvaluator_semantic_similarity_count": 60,
    "AnswerEvaluator_faithfulness_mean": 0.9111111111111112,
    "AnswerEvaluator_faithfulness_min": 0.0,
    "AnswerEvaluator_faithfulness_max": 1.0,
    "AnswerEvaluator_faithfulness_count": 60,
    "RetrievalEvaluator_context_match_mean": 1.0,
    "RetrievalEvaluator_context_match_min": 1.0,
    "RetrievalEvaluator_context_match_max": 1.0,
    "RetrievalEvaluator_context_match_count": 60,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8721230158730161,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.125,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 1.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 60,
    "RetrievalEvaluator_recall_at_3_mean": 0.8180555555555554,
    "RetrievalEvaluator_recall_at_3_min": 0.0,
    "RetrievalEvaluator_recall_at_3_max": 1.0,
    "RetrievalEvaluator_recall_at_3_count": 60,
    "RetrievalEvaluator_recall_at_5_mean": 0.8666666666666666,
    "RetrievalEvaluator_recall_at_5_min": 0.0,
    "RetrievalEvaluator_recall_at_5_max": 1.0,
    "RetrievalEvaluator_recall_at_5_count": 60,
    "RetrievalEvaluator_recall_at_10_mean": 0.9694444444444444,
    "RetrievalEvaluator_recall_at_10_min": 0.3333333333333333,
    "RetrievalEvaluator_recall_at_10_max": 1.0,
    "RetrievalEvaluator_recall_at_10_count": 60,
    "RetrievalEvaluator_precision_at_20_mean": 0.1944444444444445,
    "RetrievalEvaluator_precision_at_20_min": 0.06666666666666667,
    "RetrievalEvaluator_precision_at_20_max": 0.5333333333333333,
    "RetrievalEvaluator_precision_at_20_count": 60,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 60,
    "PerformanceEvaluator_retrieval_waste_mean": 0.9011111111111103,
    "PerformanceEvaluator_retrieval_waste_min": 0.7333333333333334,
    "PerformanceEvaluator_retrieval_waste_max": 0.9333333333333333,
    "PerformanceEvaluator_retrieval_waste_count": 60,
    "PerformanceEvaluator_answer_provided_mean": 1.0,
    "PerformanceEvaluator_answer_provided_min": 1.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 60,
    "per_difficulty": {
      "easy": {
        "AnswerEvaluator_exact_match_mean": 0.05,
        "AnswerEvaluator_sequence_similarity_mean": 0.4297023954041381,
        "AnswerEvaluator_word_overlap_mean": 0.347595435693152,
        "AnswerEvaluator_answer_length_ratio_mean": 1.6793054550813173,
        "AnswerEvaluator_keyword_match_mean": 0.7336111111111111,
        "AnswerEvaluator_semantic_similarity_mean": 0.8239212495515812,
        "AnswerEvaluator_faithfulness_mean": 0.8458333333333334,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 1.0,
        "RetrievalEvaluator_recall_at_3_mean": 1.0,
        "RetrievalEvaluator_recall_at_5_mean": 1.0,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_20_mean": 0.11666666666666665,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.9333333333333333,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "medium": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.31192621125753184,
        "AnswerEvaluator_word_overlap_mean": 0.20922409916804335,
        "AnswerEvaluator_answer_length_ratio_mean": 1.689320400065182,
        "AnswerEvaluator_keyword_match_mean": 0.517890433869536,
        "AnswerEvaluator_semantic_similarity_mean": 0.776136151956016,
        "AnswerEvaluator_faithfulness_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8467261904761905,
        "RetrievalEvaluator_recall_at_3_mean": 0.85,
        "RetrievalEvaluator_recall_at_5_mean": 0.85,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_20_mean": 0.1333333333333334,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.9333333333333333,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "hard": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.10417258938403309,
        "AnswerEvaluator_word_overlap_mean": 0.15451891703266613,
        "AnswerEvaluator_answer_length_ratio_mean": 1.8974175824175823,
        "AnswerEvaluator_keyword_match_mean": 0.382714410086682,
        "AnswerEvaluator_semantic_similarity_mean": 0.7676692769208631,
        "AnswerEvaluator_faithfulness_mean": 0.8875,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7696428571428571,
        "RetrievalEvaluator_recall_at_3_mean": 0.6041666666666666,
        "RetrievalEvaluator_recall_at_5_mean": 0.7499999999999999,
        "RetrievalEvaluator_recall_at_10_mean": 0.9083333333333334,
        "RetrievalEvaluator_precision_at_20_mean": 0.33333333333333337,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.8366666666666671,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      }
    },
    "success_rate": 1.0,
    "avg_response_time": 14.275572097301483
  }
}