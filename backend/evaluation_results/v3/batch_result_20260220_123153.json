{
  "batch_id": "batch_20260220_121452",
  "timestamp": "2026-02-20T12:14:52.885403",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John emerged unscathed, but his best friend, Tommy, suffered a broken arm, and the bus driver was hospitalized.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 9.51178503036499,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7837150127226463,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6470588235294118,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9393939393939394,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.85,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9814654955927329,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.51178503036499
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The provided context mentions that the fire was caused by \"a faulty wire in one of his beloved clocks.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 9.78434705734253,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7457627118644068,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.5714285714285714,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2666666666666666,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8571428571428571,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9318043125372582,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.78434705734253
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "According to the context, when John Doe was twenty-one and working as a junior assistant in the Evergreen Public Library, the head librarian, Mrs. Hargrove, called him into her office on a gray Thursday afternoon. She informed him that his contract would not be renewed due to budget cuts, and three positions were being eliminated, including his.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 10.369150161743164,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2742616033755274,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.21666666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7915639343460903,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.369150161743164
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "The provided context does not contain information about what happened at John's wedding. It mentions that Lila called him the night before and said she couldn't go through with it, but it doesn't describe the actual event or its outcome.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 10.406977415084839,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.02557544757033248,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.12727272727272726,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.3793103448275863,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.375,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6466103211704793,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.406977415084839
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How old was John when the school bus accident happened?",
      "expected_answer": "John was fifteen when the school bus accident occurred.",
      "generated_answer": "The provided context mentions that John was fifteen years old at the time of the school bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 9.056400060653687,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5641025641025641,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3157894736842105,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9137590092905106,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.056400060653687
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What injury did Tommy suffer in the bus accident?",
      "expected_answer": "Tommy suffered a broken arm in the bus accident.",
      "generated_answer": "The provided context mentions that Tommy suffered a broken arm in the school bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 9.68045973777771,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.6956521739130435,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6428571428571429,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6666666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.880586522558207,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.68045973777771
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John call the club he started after the bus accident?",
      "expected_answer": "John started a 'Bus Philosophy Club' after the bus accident.",
      "generated_answer": "The provided context does not contain information about John starting a club after the bus accident.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 9.70408320426941,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5125,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3157894736842105,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.42857142857142855,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7536271151375509,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.70408320426941
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John use to dry his books after the bus accident?",
      "expected_answer": "John dried his books with a hairdryer after the bus accident.",
      "generated_answer": "According to the context, John used a hairdryer to dry his books.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 9.511239290237427,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.4444444444444444,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.29411764705882354,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0909090909090908,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.42857142857142855,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8012643117310982,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.511239290237427
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What profession did John have when his house burned down?",
      "expected_answer": "John worked as a librarian when his house burned down.",
      "generated_answer": "The provided context does not contain information about what profession John had at the time of his house burning down. However, it is mentioned that he worked as a librarian in \"The Ordinary Tuesday\" section, but this is not directly related to the event of his house burning down.\n\nHowever, based on the context from \"The Calm Storm of John Doe\", it can be inferred that John was a librarian at some point in his life.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 11.144890308380127,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13924050632911392,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14545454545454545,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8385755015715655,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.144890308380127
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John collect that caused the house fire?",
      "expected_answer": "John collected antique clocks, and a faulty wire in one caused the house fire.",
      "generated_answer": "The provided context mentions that John collected antique clocks. It is stated that \"one of his beloved clocks\" had a faulty wire, which caused the house to catch fire.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 9.862725019454956,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5853658536585366,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.28125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8888888888888888,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9078068274902311,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.862725019454956
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do with the pigeons after the house fire?",
      "expected_answer": "John fed crumbs from a stale sandwich to pigeons after the house fire.",
      "generated_answer": "John sat on a bench in the local park, feeding pigeons crumbs from a stale sandwich he'd forgotten in his car.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 9.682714462280273,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5222222222222223,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.6153846153846154,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7942101311472592,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.682714462280273
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What type of clocks did John start collecting after the fire?",
      "expected_answer": "John started collecting digital clocks after the fire.",
      "generated_answer": "The provided context does not explicitly state what type of clocks John started collecting after the fire. However, it mentions that he started a new clock collection, \"this time digital ones that couldn't spark.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 10.172810077667236,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2546816479400749,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25806451612903225,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8511590151486046,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.172810077667236
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What book was John packing last in the archive?",
      "expected_answer": "John was packing a 17th-century edition of Montaigne\u2019s Essays last.",
      "generated_answer": "The context mentions that John spent the last three weeks of the archive's open life packing volumes into acid-free boxes. On the final day, he was alone in the dim basement room, sealing the last carton. Inside it lay a 17th-century edition of Montaigne's *Essays*. \n\nTherefore, the book John packed last in the archive was a 17th-century edition of Montaigne's *Essays*.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 11.183869361877441,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.22779043280182232,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15555555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7816370621833681,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.183869361877441
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What phrase was in the last book John packed?",
      "expected_answer": "The phrase 'Que s\u00e7ay-je?' (What do I know?) was in the last book.",
      "generated_answer": "The provided context mentions that inside the last carton John packed was a 17th-century edition of Montaigne's *Essays*. The page he opened to read had the following phrase:\n\n*\u201cQue s\u00e7ay-je?\u201d* \u2014 What do I know?",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 9.866209268569946,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.24,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1282051282051282,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2857142857142857,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7979530767682562,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.866209268569946
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How long did John spend packing volumes in the archive?",
      "expected_answer": "John spent the last three weeks packing volumes in the archive.",
      "generated_answer": "The context mentions that John spent \"the last three weeks\" of the archive's open life packing volumes into acid-free boxes. \n\nNote: The exact number of days is not specified, only that it was for three weeks.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 10.130225658416748,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.375,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.20588235294117646,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7101441448468255,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.130225658416748
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John keep from the archive as a private act?",
      "expected_answer": "John kept the call number of Montaigne\u2019s Essays as a private act.",
      "generated_answer": "The provided context does not contain information to answer this question about \"The Calm Storm of John Doe\" or \"The Library of Small Disappointments\". However, it can be answered based on \"The Last Book\".\n\nJohn kept the call number of Montaigne's *Essays* as a small, private act of defiance against total digitization.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 10.759587287902832,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.2961038961038961,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.75,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7578732804512356,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.759587287902832
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Who was the head librarian at Evergreen Public Library?",
      "expected_answer": "Mrs. Hargrove was the head librarian at Evergreen Public Library.",
      "generated_answer": "Mrs. Hargrove was the head librarian at Evergreen Public Library.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 10.00773310661316,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 1.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 1.0000000000000013,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.00773310661316
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How many positions were eliminated at the library?",
      "expected_answer": "Three positions were eliminated at the library.",
      "generated_answer": "The head librarian, Mrs. Hargrove\u2014a woman who treated overdue fines with the gravity of international law\u2014called him into her office on a gray Thursday afternoon.\n\nShe slid a folded letter across the desk. \u201cI\u2019m afraid we won\u2019t be renewing your contract next semester,\u201d she said, voice flat. \u201cBudget cuts. Last in, first out.\u201d\n\nJohn read the letter once, then refolded it with the same precision he used for damaged book spines. He looked up. \u201cHow many positions are being eliminated?\u201d\n\n\u201cThree. Yours included.\u201d",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 11.542826414108276,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.025134649910233394,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.02531645569620253,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6878855754006898,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.542826414108276
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was the rare-book dealer's unusual requirement?",
      "expected_answer": "The rare-book dealer required a handwritten essay on why dust is noble.",
      "generated_answer": "The rare-book dealer required applicants to submit a handwritten essay on why dust is noble.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 9.693451642990112,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8711656441717791,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.8,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.961191191895858,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.693451642990112
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John note about dust in his new job?",
      "expected_answer": "John decided that the dust was rather noble in his new job.",
      "generated_answer": "The provided context does not contain information on what John noted about dust in his new job as cataloguing assistant at the private rare-book dealer. However, it is mentioned that he decided \"the dust, he decided, really was rather noble.\"",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 10.033440351486206,
      "retrieval_count": 15,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13953488372093023,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8571428571428571,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.81647182429121,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.033440351486206
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "The provided context mentions that when the school bus accident occurred, chaos ensued with screams, shattered glass, and the acrid smell of burnt rubber. However, John emerged unscathed but his best friend Tommy suffered a broken arm, and the bus driver was hospitalized.\n\nIn contrast to others who were shaken by the incident, John sat on the curb, observing the scene calmly. When approached by a paramedic, he replied with a shrug: \"Scary? It's just the universe playing bumper cars with reality.\" He also noted in his journal that accidents are life's way of reminding us that control is an illusion.\n\nJohn's reaction was described as turning the tragedy into quiet absurdity, starting a \"Bus Philosophy Club\" at school where members discussed existentialism over vending machine snacks.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 11.774571180343628,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.15701668302257116,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1271186440677966,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.46153846153846156,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7454364085029233,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.774571180343628
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John reacted calmly. He stood outside his burning house, watching the flames with a sense of detachment. When asked by a neighbor what he would do, he replied that \"the house has decided to retire early\" and joked that he had thought about it himself on Mondays. He then pulled out a notebook and jotted down: \"Note to self: Fire sales are literal.\"",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 10.860100984573364,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.09140767824497258,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.17142857142857143,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8075346954487629,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.860100984573364
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The university archive closed because the university decided to digitize everything and close the physical stacks. This decision allowed for \"preservation\" of the rare books in climate-controlled storage, with access available by appointment only through high-resolution scans viewed on screens.\n\nJohn was packing a 17th-century edition of Montaigne's *Essays* when he finished sealing the last carton.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 10.655375719070435,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.44776119402985076,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.31666666666666665,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.7142857142857143,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8140841589585082,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.655375719070435
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "The provided context mentions that John received a class-action settlement notice related to a chemical plant leak into the river that supplied Evergreen's water. The notice offered modest compensation for \"potential long-term health effects.\"",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 10.220662593841553,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.41626794258373206,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3684210526315789,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4166666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.75,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9019458732173837,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.220662593841553
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "According to the provided context in \"The Quiet Clockwork of John Doe,\" John's sister Mara was diagnosed with an aggressive form of leukemia and had only months to live. However, it is not mentioned what ultimately happened to her. The story jumps forward to John visiting her at the hospital, where she is described as looking smaller than he remembered, swallowed by white sheets and tubes, but still holding a sharp amusement in her eyes.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 11.278031826019287,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.10784313725490197,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18666666666666668,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.45,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7863440370244574,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.278031826019287
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "The provided context mentions that John Doe was thirty-two when the diagnosis came, delivered by an oncologist. The diagnosis was pancreatic cancer, Stage IV.\n\nJohn listened to the doctor's measured sentences, nodded once, and asked only one question: \"How long until the coffee stops tasting like coffee?\" He then left the office carrying a pamphlet titled *Living with Advanced Illness*.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 11.051164865493774,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.200339558573854,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.21212121212121213,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.935483870967742,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8576567366085805,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.051164865493774
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do the day after losing his library job?",
      "expected_answer": "The day after losing his job, John sat at his desk and wrote notes, then planned to apply for three more jobs, one of which he expected to be ridiculous.",
      "generated_answer": "The provided context does not contain information about what John did the day after losing his library job. It mentions that he left the office, returned to the circulation desk, and spent the rest of his shift reshelving returns with the same quiet rhythm as always. However, it does not specify his activities for the next day.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 10.567539930343628,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.17842323651452283,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1746031746031746,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.25,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.732150819809174,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.567539930343628
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did Mara die and what was John's immediate action?",
      "expected_answer": "Mara died quietly while John was reading to her from a chapter on the indifference of the stars. He finished the paragraph before closing the book.",
      "generated_answer": "The provided context does not contain information about Mara's death or John's immediate action regarding it.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 9.78687834739685,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.203125,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.02631578947368421,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.6153846153846154,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.13333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6805561182073776,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.16666666666666666,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 6
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.78687834739685
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say about fear to Mara?",
      "expected_answer": "John said to Mara that fear is just the body\u2019s last honest opinion, but she would find out whether the absurd has an ending or just another act.",
      "generated_answer": "The provided context does not contain information about John saying anything about fear to someone named Mara.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 9.96028757095337,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.25984251968503935,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10526315789473684,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.6071428571428571,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.16666666666666666,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.658912946372666,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.125,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 8
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.96028757095337
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do with Mara's hospital bracelet?",
      "expected_answer": "John added Mara's hospital bracelet to his mementos box labeled 'Act III' after her death.",
      "generated_answer": "He added one more item to the small wooden box where he kept mementos: Mara\u2019s hospital bracelet. He labeled it simply \u201cAct III\u201d and closed the lid.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 10.084030151367188,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.48945147679324896,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14705882352941177,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.45454545454545453,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9013049084732232,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.084030151367188
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did the oncologist describe John's prognosis?",
      "expected_answer": "The oncologist described John's prognosis as statistics that hovered between 'slim' and 'theoretical' for stage IV pancreatic cancer.",
      "generated_answer": "The context mentions that the oncologist spoke in measured sentences, offering statistics that hovered somewhere between \"slim\" and \"theoretical.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 10.02850079536438,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5591397849462365,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.25,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0555555555555556,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.38461538461538464,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8112808587498033,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.02850079536438
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What adjustments did John make after his diagnosis?",
      "expected_answer": "John adjusted his routine by fifteen minutes each morning to accommodate the new fatigue and continued his routines with minor adjustments.",
      "generated_answer": "The provided context mentions that after his diagnosis, John made the following adjustment:\n\n\"He simply adjusted his routine by fifteen minutes each morning to accommodate the new fatigue that arrived like an uninvited houseguest who refused to leave the couch.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 10.120185613632202,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.45885286783042395,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.35,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9047619047619047,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6428571428571429,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8715572287879816,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.120185613632202
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What note did John leave in his cupboard?",
      "expected_answer": "John left a note saying: 'If you\u2019re reading this, the coffee is probably stale. Make a new pot anyway. Life is absurd enough without drinking yesterday\u2019s grounds.'",
      "generated_answer": "The provided context mentions that Elena found a single sheet of paper taped inside the cupboard above the coffee maker with John's neat handwriting. The note read:\n\n\"If you\u2019re reading this, the coffee is probably stale. Make a new pot anyway. Life is absurd enough without drinking yesterday\u2019s grounds.\"",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 10.549288034439087,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.6038543897216274,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.425531914893617,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8148148148148149,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8421052631578947,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9050364251642744,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.549288034439087
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John do after the wedding was called off?",
      "expected_answer": "John went to the church, waited twenty minutes for a miracle, then walked out, bought a pretzel, ate half, and fed the rest to pigeons.",
      "generated_answer": "According to the context, \"At ten minutes past the hour the ceremony was meant to begin, John stood on the steps watching guests trickle away in polite confusion. An aunt patted his arm. A cousin offered awkward condolences. John thanked them each with the same gentle nod.\"\n\nWhen the last car pulled away, he walked inside the empty church. The altar flowers were still fresh; the programs lay stacked on a side table. He sat in the front pew, placed the rose beside him, and waited another twenty minutes\u2014long enough for any last-minute miracle to arrive fashionably late.\n\nNone did.\n\nHe stood, brushed invisible lint from his sleeve, and walked out into bright noon sunlight. On the sidewalk he met the street vendor again, now selling pretzels.\"\n\nSo, after the wedding was called off, John:\n\n* Watched guests leave\n* Sat in the empty church for 20 minutes\n* Walked out of the church\n* Met the street vendor and bought a pretzel",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 13.73086142539978,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.07692307692307693,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08661417322834646,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4666666666666667,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7103017682573678,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.73086142539978
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say to the vendor about the rose?",
      "expected_answer": "John said the rose worked perfectly, as it reminded him that beauty doesn\u2019t always come with permanence.",
      "generated_answer": "\"It worked perfectly,\" John replied. \"It just reminded me that beauty doesn\u2019t always come with permanence.\"",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 9.56091022491455,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7867298578199052,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.391304347826087,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9411764705882353,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.8461538461538461,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8616342898452551,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.56091022491455
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John keep from the archive and why?",
      "expected_answer": "John kept a slip of paper with the call number of Montaigne\u2019s Essays as a small, private act of defiance against total digitization.",
      "generated_answer": "The provided context does not contain information to answer this question about what John kept from the archive and why. However, it mentions that he slipped a single slip of paper into his pocket with the call number of Montaigne's *Essays*, which suggests that he may have been keeping some record or reference to specific books.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 10.68380093574524,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.23326133909287258,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.16666666666666666,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6878497046488845,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.68380093574524
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John say about missing the books?",
      "expected_answer": "John said missing is just another way of remembering, and remembering is free.",
      "generated_answer": "John said, \"I already do\" (missing the books), but then added, \"But missing is just another way of remembering. And remembering is free.\"",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 10.05375623703003,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7162790697674418,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.36,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.7692307692307692,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.863197097008968,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.05375623703003
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John use the settlement money for?",
      "expected_answer": "John used the settlement money to buy a better coffee maker.",
      "generated_answer": "The provided context does not contain information on what John used the settlement money for. It only mentions that he read the amount and set the letter on the counter, but there is no further mention of how he utilized the funds.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 10.430360794067383,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.24054982817869416,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.12195121951219512,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.663859283932035,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.430360794067383
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What did John think about fighting entropy?",
      "expected_answer": "John thought fighting entropy is like yelling at gravity\u2014undignified.",
      "generated_answer": "The provided context does not contain information about John's thoughts on fighting entropy. However, it mentions that he noted in his journal: \"Accidents are life's way of reminding us that control is an illusion. Why fight the farce?\" This suggests that John has a philosophical perspective on accepting and adapting to the inevitability of chaos and disorder (entropy) rather than trying to resist or fight it.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 11.034177541732788,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.024896265560165973,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.047619047619047616,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.5714285714285714,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7452187166600478,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.14285714285714285,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 7
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.06666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.034177541732788
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John cope with the house fire and what metaphor did he use?",
      "expected_answer": "John coped by accepting it calmly, using the metaphor that life's a library where books burn but stories float around waiting for new shelves.",
      "generated_answer": "The provided context mentions that after the house fire, John stood calmly observing the ruins, then pulled out a small notebook and jotted down \"Note to self: Fire sales are literal.\" He also wandered to the local park, feeding pigeons crumbs from a stale sandwich. When asked how he coped so calmly, he'd shrug and say, \"Tragedy is just the universe's way of editing the plot. Why rage against the red pen?\"\n\nJohn used the metaphor that tragedy is like the universe's editing process, comparing it to a writer using a red pen to edit a manuscript.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 11.02259874343872,
      "retrieval_count": 15,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04052098408104197,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10112359550561797,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.35294117647058826,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6753793260023377,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.02259874343872
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "The provided context does not contain information about what specific jobs John applied for before taking the rare-book job at a private dealer. However, it mentions that he applied for three more jobs in total:\n\n1. Night security at a self-storage facility.\n2. Part-time barista at a shop that only served oat-milk lattes.\n3. Cataloguing assistant at a private rare-book dealer who insisted applicants submit a handwritten essay on why dust is noble.\n\nJohn took the third job, which was cataloguing assistant at the private rare-book dealer.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 10.810369729995728,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.37244897959183676,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4057971014492754,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.84,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8167485956426632,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.16666666666666666,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.13333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.810369729995728
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "The provided context does not contain information on how John handled his sister's death after she passed away in the hospital. It only describes their interactions before her passing, including his calm and philosophical demeanor during her illness.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 9.647646188735962,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04932735426008968,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09836065573770492,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0857142857142856,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.05,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7278593989574758,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.647646188735962
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "John's last requests were for a fresh pot of truly terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 9.970478296279907,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8831168831168831,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.76,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.9050589813693819,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 9.970478296279907
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "The provided context does not contain a comprehensive answer to this question, as there are multiple instances of John facing different tragedies in various stages of his life. However, based on the available information:\n\nThroughout his life, John seemed to view tragedy with a sense of detachment and absurdity. He often responded to catastrophic events with philosophical musings, comparing them to plot twists or bad comedy.\n\nIn \"The Absurd Adolescence of John Doe,\" he described accidents as life's way of reminding us that control is an illusion (p. 4). He also started a \"Bus Philosophy Club\" at school, where members discussed existentialism over vending machine snacks (p. 5).\n\nIn \"The Calm Storm of John Doe,\" when his home caught fire, he observed the ruins and said, \"It seems the house has decided to retire early. Can't blame it\u2014I've thought about it myself on Mondays\" (p. 7). He also noted in his journal: \"Fire sales are literal\" (p. 8).\n\nIn \"The Uninvited Guest,\" when diagnosed with pancreatic cancer, he asked how long until the coffee would stop tasting like coffee and later said that fear is just the body's last honest opinion (p. 14-15). He also compared his life to a play where everyone knows the ending.\n\nJohn's philosophy about tragedy can be summarized as: tragedies are inevitable, control is an illusion, and life is absurd.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 15.542354106903076,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1070528967254408,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09714285714285714,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.34782608695652173,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7456237978819267,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 15.542354106903076
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "John had several jobs throughout his life, including:\n\n1. Junior assistant at the Evergreen Public Library (at age 21)\n2. Cataloguing assistant at a private rare-book dealer\n3. Part-time barista at a shop that only served oat-milk lattes\n4. Night security at a self-storage facility\n5. Librarian (shelving books and observing the stories they whispered)\n\nLater in life, he owned his own small secondhand bookstore.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 11.199854135513306,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.1040650406504065,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1791044776119403,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.6,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7774018169247598,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 4,
            "expected_count": 4,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt",
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.199854135513306
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.7333333333333334,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's reactions to the bus accident and house fire reflect his philosophy?",
      "expected_answer": "In both the bus accident and house fire, John reacted with philosophical detachment and calm acceptance, seeing them as the universe's absurd events rather than personal tragedies, starting a philosophy club after the accident and rebuilding deliberately after the fire.",
      "generated_answer": "The provided context does not contain a direct comparison between John's reactions to the bus accident and house fire. However, it mentions that in both incidents, John remained calm and observed the situation with a philosophical perspective.\n\nIn the bus accident, John said, \"Scary? It's just the universe playing bumper cars with reality.\" He also noted in his journal: \"Accidents are life's way of reminding us that control is an illusion. Why fight the farce?\"\n\nSimilarly, after the house fire, John observed, \"It seems the house has decided to retire early. Can't blame it\u2014I've thought about it myself on Mondays.\" He also jotted down: \"Note to self: Fire sales are literal.\"\n\nJohn's reactions in both situations reflect his philosophy of accepting and observing life's tragedies as part of an absurd and unpredictable universe, where control is an illusion.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 12.92762279510498,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.05291005291005291,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15789473684210525,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4827586206896552,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8563612249660963,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.92762279510498
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What common theme connects John's job loss at the library and the archive closure?",
      "expected_answer": "Both events involved budget cuts or modernization leading to job changes; John lost his junior assistant role to cuts and the archive closed for digitization, but he adapted by finding new roles in rare books and eventually owning a bookstore.",
      "generated_answer": "The provided context does not explicitly state a direct connection between John's job loss at the library and the archive closure. However, both events involve changes to institutions where John worked with books.\n\nIn \"The Library of Small Disappointments,\" John loses his job due to budget cuts, but he remains calm and matter-of-fact about it.\n\nIn \"The Last Book,\" the university decides to digitize everything and close the physical stacks, effectively ending John's work in the archive. \n\nBoth events seem to be handled by John with a sense of detachment or acceptance, as if he views them as part of life's natural flow rather than personal tragedies.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt"
      ],
      "response_time": 11.994870901107788,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03114571746384872,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1568627450980392,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.32,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8235197156184575,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.994870901107788
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's handling of his sister's death compare to his own cancer diagnosis?",
      "expected_answer": "John handled both with stoicism and calm; he finished reading to Mara and labeled her bracelet 'Act III,' and for his cancer, he adjusted routines minimally and saw it as a predictable plot.",
      "generated_answer": "The provided context does not directly compare John's handling of his sister's death and his own cancer diagnosis. However, it mentions that when his sister Mara was diagnosed with leukemia, he \"did not cry. He did not pace. He simply sat at his small kitchen table, listening to the laundromat's rhythmic thump-thump below, and thought: *Well. The wheel keeps turning.*\"\n\nIn contrast, when John received his own diagnosis of pancreatic cancer, he asked only one question: \"How long until the coffee stops tasting like coffee?\" He then adjusted his routine by fifteen minutes each morning to accommodate the new fatigue.\n\nThe context does not provide a direct comparison between the two situations.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 12.372201919555664,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.03153153153153153,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.10476190476190476,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2631578947368421,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6426683975136405,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.372201919555664
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What role did coffee play in John's life across events?",
      "expected_answer": "Coffee was a constant ritual; he made terrible coffee during his cancer, requested terrible coffee as a last wish, and used settlement money for a better coffee maker, symbolizing normalcy amid tragedy.",
      "generated_answer": "The provided context mentions the following instances where coffee played a role in John's life:\n\n* In \"The Uninvited Guest,\" John asks the oncologist how long it would take for his coffee to stop tasting like coffee, indicating that he values his daily coffee routine.\n* In \"The Ordinary Tuesday,\" John makes coffee that is exactly the right shade of bitter every morning at 6:17 AM.\n* In \"The Absurd Adolescence of John Doe,\" there is no mention of John drinking coffee during his teenage years.\n\nOverall, it appears that coffee was an important part of John's daily routine and a source of comfort for him.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 11.915874719619751,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.014796547472256474,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08695652173913043,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.21739130434782608,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7763158315455672,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.915874719619751
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's wedding cancellation and job losses show his resilience?",
      "expected_answer": "John treated the wedding cancellation like a plot twist, feeding pigeons afterward, and after job losses from budget cuts and archive closure, he applied for new roles and adapted without distress.",
      "generated_answer": "The provided context does not contain information about John's job losses showing his resilience. However, it mentions that when Lila cancelled their wedding, John said he would still be at the church \"just in case the panic changes its mind.\" This shows that despite the cancellation, John remained calm and understanding, indicating his resilience in handling unexpected situations.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt"
      ],
      "response_time": 10.797725439071655,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04475043029259897,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08108108108108109,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.8709677419354838,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.18181818181818182,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7673118505843328,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.797725439071655
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What environmental event linked to John's settlement, and how did it tie to his health?",
      "expected_answer": "The chemical plant leak into Evergreen's water supply led to the settlement for potential health effects, which subtly connected to his later cancer diagnosis, though he accepted both without fury.",
      "generated_answer": "The provided context mentions that there was a chemical plant upstream from Evergreen that leaked into the river supplying the town's water years earlier. This leak is linked to John's settlement, as he receives notice of a class-action settlement for \"potential long-term health effects\" due to the contamination.\n\nHowever, it does not fully explain how this event ties directly to his health or if he has any specific health issues related to the environmental incident.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 11.161221265792847,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08669656203288491,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15384615384615385,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.4090909090909091,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.839237979214205,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 2,
            "expected_count": 2,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.161221265792847
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8666666666666667,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 2
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's philosophy evolve from adolescence to death?",
      "expected_answer": "From seeing accidents as 'bumper cars with reality' in adolescence, to accepting fire as 'editing the plot,' to viewing his cancer as a 'predictable plot' with curiosity about the ending, John consistently embraced life's absurdity.",
      "generated_answer": "The provided context does not contain information on how John's philosophy evolved from adolescence to death in detail. However, it can be inferred that his views remained consistent with the absurdism and acceptance of life's unpredictability he exhibited as a teenager.\n\nAs a teenager (in \"The Absurd Adolescence of John Doe\"), John observed life's tragedies with a detached perspective, viewing them as mere plot twists in an absurd narrative. He noted that accidents remind us control is an illusion and suggested accepting this reality.\n\nIn his mid-twenties (\"The Quiet Clockwork of John Doe\") and later when diagnosed with pancreatic cancer (\"The Uninvited Guest\"), John continued to apply this philosophical outlook. He saw life's challenges as part of the universe's \"bad comedy\" and accepted them with a calm, stoic demeanor.\n\nThroughout these periods, his philosophy remained centered on the idea that life is inherently absurd and unpredictable, and that one should learn to accept and even find humor in its tragedies.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 13.38463044166565,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.025356576862123614,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.11023622047244094,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.36,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7889099688456165,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.38463044166565
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What mementos did John keep from family tragedies?",
      "expected_answer": "From his sister's death, John kept her hospital bracelet in his 'Act III' mementos box; tragedies like parents' accident and sister's death shaped his stoic philosophy.",
      "generated_answer": "The provided context does not contain information to answer this question about what specific mementos John kept from family tragedies. However, it mentions that he kept a hospital bracelet labeled \"Act III\" after his sister Mara's diagnosis and treatment for leukemia.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 10.108679294586182,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.032036613272311214,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14545454545454545,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.5769230769230769,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.35,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.8099697299164768,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 1,
            "expected_count": 1,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.2,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.108679294586182
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.9333333333333333,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did books and libraries thread through John's career setbacks?",
      "expected_answer": "John's library job loss led to rare-book cataloguing, then university archive work ending in digitization, culminating in owning a bookstore; books were constant amid changes.",
      "generated_answer": "The provided context does not contain information about how books and libraries specifically threaded through John's career setbacks. However, it mentions that John worked as a librarian in the Evergreen Public Library and later catalogued rare books at the university archive.\n\nIt can be inferred from the text that John has a deep connection to books and libraries, but the specific ways in which they relate to his career setbacks are not explicitly stated.",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 11.358590364456177,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.05669291338582677,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09333333333333334,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3157894736842105,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7640466175215321,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.26666666666666666,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.358590364456177
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What ironies did John note in his life events?",
      "expected_answer": "Ironies like a clock causing a fire, digitization closing an archive of physical books, and a water pollution settlement possibly linking to his cancer, all accepted as life's absurdities.",
      "generated_answer": "The provided context does not explicitly mention specific ironies noted by John. However, it can be inferred that he views various aspects of his life as ironic:\n\n*   He notes that \"Accidents are life's way of reminding us that control is an illusion\" after his sister's diagnosis.\n*   He describes the universe as running on \"bad comedy\" and says that tragedies are just the punchline arriving late.\n*   When receiving notice of a class-action settlement, he comments that the universe has finally sent an invoice, implying that life's challenges can be seen as mere paperwork or bureaucratic tasks.\n*   After his home catches fire, he observes that the house has decided to retire early, suggesting that he views this event as a natural part of life's cycle.\n\nThese statements suggest that John has developed a philosophical outlook on life, seeing irony and absurdity in various events.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 12.925119876861572,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.033426183844011144,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08403361344537816,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.21052631578947367,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7298990326728646,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.16666666666666666,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 4,
            "expected_count": 4,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt",
              "the-last-book.txt",
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.75,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.925119876861572
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.7333333333333334,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's relationships with friends and family show his detachment?",
      "expected_answer": "John offered observations instead of emotions to friends during cancer, was stoic with Mara during her death, and calmly accepted Lila's wedding cancellation.",
      "generated_answer": "The provided context does not contain explicit information on how John's relationships with friends and family showed his detachment. However, it mentions that when his parents fretted about him after the school bus accident, he methodically dried his books with a hairdryer and noted in his journal: \"Accidents are life's way of reminding us that control is an illusion.\" This suggests that John may have been detached from their emotional expectations.\n\nAdditionally, when his sister Mara was diagnosed with leukemia, John read the letter twice, folded it neatly, and placed it inside the cover of his worn copy of *The Myth of Sisyphus*. He did not cry or pace, indicating a level of detachment from the situation.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 12.121545314788818,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04342857142857143,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.06666666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.2222222222222222,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.6427611093384458,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 12.121545314788818
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What common items symbolized normalcy in John's tragedies?",
      "expected_answer": "Coffee symbolized normalcy: terrible coffee during cancer, a new maker from settlement, and pretzels/pigeons after wedding, all grounding him in routine.",
      "generated_answer": "The provided context does not contain information to answer this question about what common items symbolized normalcy in John's tragedies. However, it mentions that John used a hairdryer to dry his books after the school bus accident and that he had clocks at home which were damaged in the fire.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 10.45081901550293,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.022271714922048998,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.13333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7028740967746032,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-ordinary-tuesday.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.14285714285714285,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 7
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.3333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 10.45081901550293
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did Evergreen feature in John's life story?",
      "expected_answer": "Evergreen was the suburb of his adolescence, home to his library job, water pollution settlement, and where he owned his bookstore amid various tragedies.",
      "generated_answer": "The provided context does not contain a comprehensive answer to how Evergreen featured in John's life story. However, it mentions that:\n\n* In \"The Quiet Clockwork of John Doe,\" John lived in a one-room apartment above a laundromat in Evergreen.\n* In \"The Absurd Adolescence of John Doe,\" John navigated his teenage years in the bustling suburb of Evergreen during the late 1990s.\n* In \"The Uninvited Guest,\" there is no mention of Evergreen, but it is mentioned that John worked at a university archive and later had friends who visited him.\n* In \"The Calm Storm of John Doe,\" John lived in a modest home in the quiet town of Evergreen.\n* In \"The Ordinary Tuesday,\" John owned a small secondhand bookstore in Evergreen.\n\nEvergreen seems to be a recurring location throughout John's life, serving as both a backdrop for his personal growth and struggles.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 13.411602020263672,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.023833167825223437,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0970873786407767,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.3333333333333333,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.716960768231435,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-absurd-adolescence-of-john-doe.txt",
              "the-library-of-small-disappointments.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 2
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.3333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 13.411602020263672
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What role did reading play in John's coping mechanisms?",
      "expected_answer": "John read philosophy to Mara, continued reading during chemo, and packed Montaigne's Essays last, using books to process absurdity and mortality.",
      "generated_answer": "Reading played a significant role in John's coping mechanisms. He often read philosophical texts, such as Camus' *The Myth of Sisyphus*, to find meaning and perspective on life's challenges. After the school bus accident, he noted in his journal that accidents are life's way of reminding us that control is an illusion, suggesting that reading had helped him develop a philosophical outlook. Later, when facing his sister's illness and then his own diagnosis, John continued to read and find solace in the words of others, using them to frame his experiences and maintain a sense of calm acceptance.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-ordinary-tuesday.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt"
      ],
      "response_time": 11.957632780075073,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.042953020134228186,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.08139534883720931,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.29411764705882354,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7455143816069232,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 0.25,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-last-book.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.25,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 4
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.4666666666666667,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.957632780075073
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John's possessions reflect his philosophy?",
      "expected_answer": "Clocks (causing fire), mementos box ('Act III'), philosophy books, and coffee rituals all embodied acceptance of time, memory, and life's indifference.",
      "generated_answer": "John's possessions reflected his philosophy by being reminders of impermanence and the absurdity of life. His collection of antique clocks, which were destroyed in the fire, symbolized his belief that time is the only constant, but even it can be affected by chaos (e.g., a faulty wire). The fact that he kept mementos, such as Mara's hospital bracelet, in a small wooden box labeled \"Act III,\" suggests that he views life as a play with its own narrative arc. His journal entries and notes to himself also reflect his philosophical outlook, often incorporating humor and absurdity to cope with adversity.\n\nThe provided context does not fully explain the significance of each specific item or how they all relate to John's philosophy.",
      "retrieved_docs": [
        "the-quiet-clockwork-of-john-doe.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 11.42236614227295,
      "retrieval_count": 15,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04971751412429379,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.045871559633027525,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.1875,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "semantic_similarity",
          "value": 0.7730086431978795,
          "confidence": 0.95,
          "metadata": {
            "method": "OllamaEmbedder",
            "model": "nomic-embed-text:latest"
          }
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "faithfulness",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "judge_model": "llama3.1:8b"
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_count": 3,
            "expected_count": 3,
            "matched_files": [
              "the-calm-storm-of-john-doe.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-uninvited-guest.txt"
            ]
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.95,
          "metadata": {
            "first_relevant_rank": 1
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_3",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 3
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_5",
          "value": 0.6666666666666666,
          "confidence": 0.9,
          "metadata": {
            "k": 5
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "recall_at_10",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": {
            "k": 10
          }
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "precision_at_20",
          "value": 0.5333333333333333,
          "confidence": 0.85,
          "metadata": null
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 11.42236614227295
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_waste",
          "value": 0.8,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 15,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 60,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.016666666666666666,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 1.0,
    "AnswerEvaluator_exact_match_count": 60,
    "AnswerEvaluator_sequence_similarity_mean": 0.28537163095537027,
    "AnswerEvaluator_sequence_similarity_min": 0.014796547472256474,
    "AnswerEvaluator_sequence_similarity_max": 1.0,
    "AnswerEvaluator_sequence_similarity_count": 60,
    "AnswerEvaluator_word_overlap_mean": 0.2354539562159786,
    "AnswerEvaluator_word_overlap_min": 0.02531645569620253,
    "AnswerEvaluator_word_overlap_max": 1.0,
    "AnswerEvaluator_word_overlap_count": 60,
    "AnswerEvaluator_answer_length_ratio_mean": 1.752297803987481,
    "AnswerEvaluator_answer_length_ratio_min": 0.6071428571428571,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 60,
    "AnswerEvaluator_keyword_match_mean": 0.5342997855294198,
    "AnswerEvaluator_keyword_match_min": 0.05,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 60,
    "AnswerEvaluator_semantic_similarity_mean": 0.7939813665593883,
    "AnswerEvaluator_semantic_similarity_min": 0.6426683975136405,
    "AnswerEvaluator_semantic_similarity_max": 1.0000000000000013,
    "AnswerEvaluator_semantic_similarity_count": 60,
    "AnswerEvaluator_faithfulness_mean": 0.8972222222222223,
    "AnswerEvaluator_faithfulness_min": 0.0,
    "AnswerEvaluator_faithfulness_max": 1.0,
    "AnswerEvaluator_faithfulness_count": 60,
    "RetrievalEvaluator_context_match_mean": 1.0,
    "RetrievalEvaluator_context_match_min": 1.0,
    "RetrievalEvaluator_context_match_max": 1.0,
    "RetrievalEvaluator_context_match_count": 60,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8721230158730161,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.125,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 1.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 60,
    "RetrievalEvaluator_recall_at_3_mean": 0.8180555555555554,
    "RetrievalEvaluator_recall_at_3_min": 0.0,
    "RetrievalEvaluator_recall_at_3_max": 1.0,
    "RetrievalEvaluator_recall_at_3_count": 60,
    "RetrievalEvaluator_recall_at_5_mean": 0.8666666666666666,
    "RetrievalEvaluator_recall_at_5_min": 0.0,
    "RetrievalEvaluator_recall_at_5_max": 1.0,
    "RetrievalEvaluator_recall_at_5_count": 60,
    "RetrievalEvaluator_recall_at_10_mean": 0.9694444444444444,
    "RetrievalEvaluator_recall_at_10_min": 0.3333333333333333,
    "RetrievalEvaluator_recall_at_10_max": 1.0,
    "RetrievalEvaluator_recall_at_10_count": 60,
    "RetrievalEvaluator_precision_at_20_mean": 0.1944444444444445,
    "RetrievalEvaluator_precision_at_20_min": 0.06666666666666667,
    "RetrievalEvaluator_precision_at_20_max": 0.5333333333333333,
    "RetrievalEvaluator_precision_at_20_count": 60,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 60,
    "PerformanceEvaluator_retrieval_waste_mean": 0.9011111111111103,
    "PerformanceEvaluator_retrieval_waste_min": 0.7333333333333334,
    "PerformanceEvaluator_retrieval_waste_max": 0.9333333333333333,
    "PerformanceEvaluator_retrieval_waste_count": 60,
    "PerformanceEvaluator_answer_provided_mean": 1.0,
    "PerformanceEvaluator_answer_provided_min": 1.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 60,
    "per_difficulty": {
      "easy": {
        "AnswerEvaluator_exact_match_mean": 0.05,
        "AnswerEvaluator_sequence_similarity_mean": 0.4361126847425787,
        "AnswerEvaluator_word_overlap_mean": 0.3450771206748369,
        "AnswerEvaluator_answer_length_ratio_mean": 1.6904165661924282,
        "AnswerEvaluator_keyword_match_mean": 0.7336111111111111,
        "AnswerEvaluator_semantic_similarity_mean": 0.8302794326779518,
        "AnswerEvaluator_faithfulness_mean": 0.8458333333333334,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 1.0,
        "RetrievalEvaluator_recall_at_3_mean": 1.0,
        "RetrievalEvaluator_recall_at_5_mean": 1.0,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_20_mean": 0.11666666666666665,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.9333333333333333,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "medium": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.314624294531209,
        "AnswerEvaluator_word_overlap_mean": 0.20682373621675385,
        "AnswerEvaluator_answer_length_ratio_mean": 1.675510876255658,
        "AnswerEvaluator_keyword_match_mean": 0.5133449793240815,
        "AnswerEvaluator_semantic_similarity_mean": 0.7840620700840008,
        "AnswerEvaluator_faithfulness_mean": 1.0,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.8467261904761905,
        "RetrievalEvaluator_recall_at_3_mean": 0.85,
        "RetrievalEvaluator_recall_at_5_mean": 0.85,
        "RetrievalEvaluator_recall_at_10_mean": 1.0,
        "RetrievalEvaluator_precision_at_20_mean": 0.1333333333333334,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.9333333333333333,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      },
      "hard": {
        "AnswerEvaluator_exact_match_mean": 0.0,
        "AnswerEvaluator_sequence_similarity_mean": 0.1053779135923234,
        "AnswerEvaluator_word_overlap_mean": 0.15446101175634522,
        "AnswerEvaluator_answer_length_ratio_mean": 1.8909659695143568,
        "AnswerEvaluator_keyword_match_mean": 0.35594326615306665,
        "AnswerEvaluator_semantic_similarity_mean": 0.7676025969162142,
        "AnswerEvaluator_faithfulness_mean": 0.8458333333333334,
        "RetrievalEvaluator_context_match_mean": 1.0,
        "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7696428571428571,
        "RetrievalEvaluator_recall_at_3_mean": 0.6041666666666666,
        "RetrievalEvaluator_recall_at_5_mean": 0.7499999999999999,
        "RetrievalEvaluator_recall_at_10_mean": 0.9083333333333334,
        "RetrievalEvaluator_precision_at_20_mean": 0.33333333333333337,
        "PerformanceEvaluator_response_time_score_mean": 0.0,
        "PerformanceEvaluator_retrieval_waste_mean": 0.8366666666666671,
        "PerformanceEvaluator_answer_provided_mean": 1.0
      }
    },
    "success_rate": 1.0,
    "avg_response_time": 10.850653553009034
  }
}