{
  "batch_id": "batch_20260201_222102",
  "timestamp": "2026-02-01T22:21:02.477647",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The bus skidded on wet pavement and crashed into a fallen tree. The impact caused screams, shattered glass and burnt\u2011rubber fumes. John survived without injury, but his best friend Tommy broke an arm and the bus driver was hospitalized.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 26.89534616470337,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.21076233183856502,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4318181818181818,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.1818181818181819,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 26.89534616470337
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "John stayed calm and detached on the curb, shrugging off the paramedic\u2019s question with a philosophical remark and later noting the incident in his journal. In contrast, his parents hovered anxiously, counselors offered group hugs, and other students replayed the horror in whispers, showing more fear and emotional reaction.",
      "retrieved_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 26.15178108215332,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.021778584392014518,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.14285714285714285,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.4411764705882353,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 26.15178108215332
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The fire was caused by a faulty electrical wire in one of John\u2019s antique clocks.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 18.464980125427246,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8441558441558441,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.7647058823529411,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-ordinary-tuesday.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 18.464980125427246
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John reacted calmly and matter\u2011of\u2011factly. He stood with his hands in his pockets, watched the smoke, and said, \u201cWell, it seems the house has decided to retire early. Can't blame it\u2014I've thought about it myself on Mondays.\u201d He was not distraught; he simply noted, in a spare notebook, \u201cNote to self: Fire sales are literal,\u201d and continued to observe the ruins with a detached, almost humorous perspective.",
      "retrieved_docs": [
        "the-calm-storm-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-library-of-small-disappointments.txt"
      ],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 31.777048110961914,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.016611295681063124,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.18840579710144928,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-calm-storm-of-john-doe.txt",
              "the-ordinary-tuesday.txt",
              "the-wedding-that-wasnt.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 31.777048110961914
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The archive closed because the university decided to digitize all holdings and shut down the physical stacks. John was packing a 17th\u2011century edition of Montaigne\u2019s *Essays* as the last book.",
      "retrieved_docs": [
        "the-last-book.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt"
      ],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 30.256770610809326,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.673469387755102,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9375,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-last-book.txt"
            ],
            "expected_docs": [
              "the-last-book.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 30.256770610809326
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "At 21, John was told by the head librarian that his contract would not be renewed\u2014he was one of three positions being cut. After the meeting he returned to the circulation desk, finished reshelving returns, closed the library, and left for home.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-library-of-small-disappointments.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 37.62819814682007,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.06989247311827956,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.09090909090909091,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.9090909090909092,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.3333333333333333,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "expected_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 37.62819814682007
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "John applied for three positions:  \n1. Night security at a self\u2011storage facility.  \n2. Part\u2011time barista at a shop that served only oat\u2011milk lattes.  \n3. Cataloguing assistant at a private rare\u2011book dealer (who required a handwritten essay on why dust is noble).",
      "retrieved_docs": [
        "the-library-of-small-disappointments.txt",
        "the-wedding-that-wasnt.txt",
        "the-last-book.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 28.853839635849,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.7619047619047619,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.40816326530612246,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.0512820512820513,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "expected_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-library-of-small-disappointments.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.853839635849
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "He received a notice of a class\u2011action settlement for a chemical plant leak that had contaminated the river supplying Evergreen\u2019s water, offering modest compensation for potential long\u2011term health effects.",
      "retrieved_docs": [
        "the-ordinary-tuesday.txt",
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 24.53494620323181,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08947368421052632,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.2894736842105263,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2083333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-ordinary-tuesday.txt"
            ],
            "expected_docs": [
              "the-ordinary-tuesday.txt"
            ],
            "retrieved_docs": [
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-library-of-small-disappointments.txt",
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 24.53494620323181
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "John\u2019s sister Mara was diagnosed with an aggressive form of leukemia. She passed away three weeks later, quietly, while John was reading to her.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-last-book.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 19.63231372833252,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5015873015873016,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.75,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-ordinary-tuesday.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 19.63231372833252
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "John stayed with Mara in room\u202f412, reading philosophy books, making terrible instant coffee, and simply sitting beside her while the IV drip ticked like a metronome. He kept a calm, almost detached presence\u2014nurses called him \u201cthe calm one,\u201d while Mara called him \u201cthe infuriating one.\u201d When Mara died, he finished the paragraph he was reading, sat with her for a long time, and listened to the machines wind down into silence. Afterward he returned to Evergreen, added her hospital bracelet to a small wooden box (labeling it \u201cAct\u202fIII\u201d), and carried the memory with the same steady, umbrella\u2011like preparedness he used for everything else.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-uninvited-guest.txt",
        "the-calm-storm-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 47.105401039123535,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.04316546762589928,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.20652173913043478,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 47.105401039123535
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "John Doe was diagnosed with stage\u202fIV pancreatic cancer. He reacted calmly: he listened, nodded once, asked a single question about coffee, then left the office with a pamphlet on living with advanced illness, without showing dramatic emotion.",
      "retrieved_docs": [
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 36.14543700218201,
      "retrieval_count": 4,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.16289592760180996,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.19230769230769232,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2258064516129032,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-uninvited-guest.txt"
            ],
            "expected_docs": [
              "the-uninvited-guest.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-ordinary-tuesday.txt",
              "the-wedding-that-wasnt.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 36.14543700218201
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "John\u2019s final requests were:\n\n1. A fresh pot of truly terrible coffee.  \n2. That someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-uninvited-guest.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 28.003761053085327,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8525641025641025,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6296296296296297,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-uninvited-guest.txt"
            ],
            "expected_docs": [
              "the-uninvited-guest.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-last-book.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.003761053085327
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "John\u2019s wedding never took place. Lila called the night before and said she couldn\u2019t go, citing her own panic. John arrived at St.\u202fAgnes Church, watched guests leave in polite confusion, sat in the front pew with a rose, and waited for a last\u2011minute miracle that never came. He eventually left the church, met the street vendor, and walked home.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-ordinary-tuesday.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 30.378779649734497,
      "retrieval_count": 4,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.14056224899598393,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.16923076923076924,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-wedding-that-wasnt.txt"
            ],
            "expected_docs": [
              "the-wedding-that-wasnt.txt"
            ],
            "retrieved_docs": [
              "the-last-book.txt",
              "the-wedding-that-wasnt.txt",
              "the-ordinary-tuesday.txt",
              "the-quiet-clockwork-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 30.378779649734497
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.25,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "John\u2019s view of tragedy was that it is an inevitable, absurd part of life that should be met with calm acceptance rather than anger. He treated misfortune as a \u201cplot edit\u201d or a farce\u2014\u201ctragedy is just the universe\u2019s way of editing the plot\u201d (The Calm Storm) and \u201caccidents are life\u2019s way of reminding us that control is an illusion\u201d (The Absurd Adolescence). He saw beauty and stories as surviving beyond physical loss (\u201cbooks burn, but the stories float around\u201d) and approached loss with stoic humor, refusing to let it disturb his routine (\u201cI tried anger once. It didn\u2019t fit. Too loud\u201d). In short, tragedy is a reminder of impermanence and the absurdity of control, and John responded by staying calm, accepting it, and carrying on.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 85.3028416633606,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.11825726141078838,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.19811320754716982,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5833333333333333,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.6666666666666666,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-calm-storm-of-john-doe.txt",
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "expected_docs": [
              "the-uninvited-guest.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": [
              "the-wedding-that-wasnt.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-absurd-adolescence-of-john-doe.txt",
              "the-calm-storm-of-john-doe.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 85.3028416633606
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.75,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "**John Doe\u2019s jobs (as recorded in the provided stories)**  \n\n| Period / Story | Position | Employer / Setting |\n|----------------|----------|--------------------|\n| \u201cThe Library of Small Disappointments\u201d | Junior assistant | Evergreen Public Library |\n| \u201cThe Library of Small Disappointments\u201d | Night\u2011security guard | Self\u2011storage facility |\n| \u201cThe Library of Small Disappointments\u201d | Part\u2011time barista | Oat\u2011milk latte shop |\n| \u201cThe Library of Small Disappointments\u201d | Cataloguing assistant | Private rare\u2011book dealer |\n| \u201cThe Uninvited Guest\u201d | Archivist / cataloguer | Archive (fragile first editions) |\n| \u201cThe Quiet Clockwork of John Doe\u201d | Cataloguer of rare books | University archive |\n\nThese are all the occupations explicitly mentioned in the context.",
      "retrieved_docs": [
        "the-wedding-that-wasnt.txt",
        "the-library-of-small-disappointments.txt",
        "the-uninvited-guest.txt",
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 76.99811506271362,
      "retrieval_count": 4,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.047866805411030174,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.13513513513513514,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.5,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 1.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.5,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-library-of-small-disappointments.txt"
            ],
            "expected_docs": [
              "the-last-book.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-ordinary-tuesday.txt",
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": [
              "the-uninvited-guest.txt",
              "the-wedding-that-wasnt.txt",
              "the-quiet-clockwork-of-john-doe.txt",
              "the-library-of-small-disappointments.txt"
            ]
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 76.99811506271362
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 1.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 4,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 15,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.0,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 0.0,
    "AnswerEvaluator_exact_match_count": 15,
    "AnswerEvaluator_sequence_similarity_mean": 0.3036631652168715,
    "AnswerEvaluator_sequence_similarity_min": 0.016611295681063124,
    "AnswerEvaluator_sequence_similarity_max": 0.8525641025641025,
    "AnswerEvaluator_sequence_similarity_count": 15,
    "AnswerEvaluator_word_overlap_mean": 0.3009291922801968,
    "AnswerEvaluator_word_overlap_min": 0.09090909090909091,
    "AnswerEvaluator_word_overlap_max": 0.7647058823529411,
    "AnswerEvaluator_word_overlap_count": 15,
    "AnswerEvaluator_answer_length_ratio_mean": 1.4660481122293267,
    "AnswerEvaluator_answer_length_ratio_min": 0.75,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 15,
    "AnswerEvaluator_keyword_match_mean": 1.0,
    "AnswerEvaluator_keyword_match_min": 1.0,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 15,
    "RetrievalEvaluator_mean_average_precision_mean": 0.7944444444444444,
    "RetrievalEvaluator_mean_average_precision_min": 0.3333333333333333,
    "RetrievalEvaluator_mean_average_precision_max": 1.0,
    "RetrievalEvaluator_mean_average_precision_count": 15,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.7888888888888889,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.3333333333333333,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 1.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 15,
    "RetrievalEvaluator_document_recall_mean": 1.0,
    "RetrievalEvaluator_document_recall_min": 1.0,
    "RetrievalEvaluator_document_recall_max": 1.0,
    "RetrievalEvaluator_document_recall_count": 15,
    "RetrievalEvaluator_context_match_mean": 0.9444444444444444,
    "RetrievalEvaluator_context_match_min": 0.5,
    "RetrievalEvaluator_context_match_max": 1.0,
    "RetrievalEvaluator_context_match_count": 15,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 15,
    "PerformanceEvaluator_retrieval_efficiency_mean": 0.3333333333333333,
    "PerformanceEvaluator_retrieval_efficiency_min": 0.25,
    "PerformanceEvaluator_retrieval_efficiency_max": 1.0,
    "PerformanceEvaluator_retrieval_efficiency_count": 15,
    "PerformanceEvaluator_answer_provided_mean": 1.0,
    "PerformanceEvaluator_answer_provided_min": 1.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 15,
    "success_rate": 1.0,
    "avg_response_time": 36.54197061856588
  }
}