{
  "batch_id": "batch_20260131_195431",
  "timestamp": "2026-01-31T19:54:31.009154",
  "pipeline_version": "1.0",
  "questions": [
    {
      "question": "What happened in the school bus accident when John was a teenager?",
      "expected_answer": "The school bus skidded on wet pavement and collided with a fallen tree. John's best friend Tommy suffered a broken arm and the bus driver was hospitalized. John emerged unscathed but philosophically unaffected.",
      "generated_answer": "The bus skidded on wet pavement and crashed into a fallen tree. The impact caused screams, shattered glass and burnt\u2011rubber fumes. John survived without injury, but his best friend Tommy broke an arm and the bus driver was hospitalized.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 33.5856077671051,
      "retrieval_count": 0,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.21076233183856502,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4318181818181818,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.1818181818181819,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 33.5856077671051
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react to the school bus accident compared to others?",
      "expected_answer": "While others were shaken and counselors offered group hugs, John reacted with philosophical detachment. He saw it as 'the universe playing bumper cars with reality' and started a 'Bus Philosophy Club' to discuss existentialism.",
      "generated_answer": "John stayed calm and detached, refusing to cry or panic. While others screamed, sobbed, and were visibly terrified, he simply observed the wreck, shrugged off the fear, and mused that the accident was \u201cjust the universe playing bumper cars with reality.\u201d He even dried his books with a hairdryer and wrote in his journal that accidents remind us that control is an illusion, showing a philosophical, unemotional reaction in contrast to the frantic responses of the other passengers.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt"
      ],
      "response_time": 38.64280343055725,
      "retrieval_count": 0,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.01692524682651622,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.16049382716049382,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 38.64280343055725
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What caused the fire that destroyed John's house?",
      "expected_answer": "The fire was caused by a faulty wire in one of his beloved antique clocks.",
      "generated_answer": "The fire was caused by a faulty wire in one of John\u2019s beloved clocks.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 58.192359924316406,
      "retrieval_count": 0,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.9090909090909091,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.8125,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9333333333333333,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 58.192359924316406
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John react when his house burned down?",
      "expected_answer": "He remained calm and philosophical, saying 'it seems the house has decided to retire early' and noting 'Fire sales are literal.' He wasn't distraught and rebuilt his life with 'absurd deliberation.'",
      "generated_answer": "John stood with his hands in his pockets, calmly watching the smoke curl into the sky. He remarked evenly that the house had \u201cdecided to retire early,\u201d noting that he had thought about it himself on Mondays. He was not distraught; he simply observed the ruins, pulled out a small notebook, and jotted, \u201cNote to self: Fire sales are literal.\u201d",
      "retrieved_docs": [],
      "expected_docs": [
        "the-calm-storm-of-john-doe.txt"
      ],
      "response_time": 49.61844038963318,
      "retrieval_count": 0,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.09647495361781076,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.16417910447761194,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.935483870967742,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-calm-storm-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 49.61844038963318
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "Why did the university archive close and what book was John packing last?",
      "expected_answer": "The university decided to digitize everything and close the physical stacks. The last book John packed was a 17th-century edition of Montaigne's Essays with the phrase 'Que s\u00e7ay-je?' (What do I know?).",
      "generated_answer": "The university closed the archive because it decided to digitize all holdings and shut down the physical stacks. John was packing a 17th\u2011century edition of Montaigne\u2019s *Essays* as his last book.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-last-book.txt"
      ],
      "response_time": 28.675581693649292,
      "retrieval_count": 0,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.6582278481012658,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3181818181818182,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.96875,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-last-book.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 28.675581693649292
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John at the library when he was 21?",
      "expected_answer": "He lost his job as a junior assistant due to budget cuts ('last in, first out'). Three positions were eliminated including his.",
      "generated_answer": "John was told his contract would not be renewed. The head librarian, Mrs.\u202fHargrove, slid him a folded letter announcing that three positions\u2014including his\u2014were being eliminated due to budget cuts. He read it, refolded it, and then returned to his desk to continue his shift, reshelving books and closing the library as usual.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 34.868558168411255,
      "retrieval_count": 0,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.030973451327433628,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.15254237288135594,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 34.868558168411255
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John apply for after losing his library job?",
      "expected_answer": "Night security at a self-storage facility, part-time barista at a shop that only served oat-milk lattes, and cataloguing assistant at a private rare-book dealer who required a handwritten essay on why dust is noble. He took the rare-book job.",
      "generated_answer": "John applied for three positions:  \n1. Night security at a self\u2011storage facility.  \n2. Part\u2011time barista at a shop that served only oat\u2011milk lattes.  \n3. Cataloguing assistant at a private rare\u2011book dealer (the one he ultimately accepted).",
      "retrieved_docs": [],
      "expected_docs": [
        "the-library-of-small-disappointments.txt"
      ],
      "response_time": 33.620970726013184,
      "retrieval_count": 0,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5821205821205822,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.27450980392156865,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.9230769230769231,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 33.620970726013184
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What settlement notice did John receive when he was 53?",
      "expected_answer": "A class-action settlement from a chemical plant that leaked into Evergreen's water supply years earlier. It offered modest compensation for potential long-term health effects.",
      "generated_answer": "John received a class\u2011action settlement notice in the mail. It explained that years earlier a chemical plant upstream had leaked into the river that supplies Evergreen\u2019s water, and it offered modest compensation for potential long\u2011term health effects.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 31.5215802192688,
      "retrieval_count": 0,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.13615023474178403,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.4,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.5416666666666667,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-ordinary-tuesday.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 31.5215802192688
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened to John's sister Mara?",
      "expected_answer": "She was diagnosed with an aggressive form of leukemia and given months to live. John traveled to be with her, read to her, and was present when she died three weeks later.",
      "generated_answer": "John\u2019s sister Mara died. She had been diagnosed with an aggressive form of leukemia and passed away quietly three weeks later while John was reading to her.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 26.33900785446167,
      "retrieval_count": 0,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.5198776758409785,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.3684210526315789,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.84375,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 26.33900785446167
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "How did John handle his sister's death?",
      "expected_answer": "He remained stoic, finished reading the paragraph he was on when she died, sat with her for a while, then added her hospital bracelet to his mementos box labeled 'Act III' and continued with life.",
      "generated_answer": "",
      "retrieved_docs": [],
      "expected_docs": [
        "the-quiet-clockwork-of-john-doe.txt"
      ],
      "response_time": 101.49326825141907,
      "retrieval_count": 0,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 101.49326825141907
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's diagnosis at age 32 and how did he react?",
      "expected_answer": "He was diagnosed with stage IV pancreatic cancer. He reacted with calm acceptance, asking only 'How long until the coffee stops tasting like coffee?' and continued his routines with minor adjustments.",
      "generated_answer": "John was diagnosed with **Stage\u202fIV pancreatic cancer**.  \nHe reacted calmly: he listened, nodded once, asked only a practical question about coffee, left the office with a pamphlet, and continued his work and routine with only minor adjustments.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 37.34264254570007,
      "retrieval_count": 0,
      "difficulty": "medium",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.36404494382022473,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.30434782608695654,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2258064516129032,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-uninvited-guest.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 37.34264254570007
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What were John's last requests?",
      "expected_answer": "A fresh pot of terrible coffee and that someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "generated_answer": "John\u2019s last requests were:  \n1. A fresh pot of truly terrible coffee.  \n2. That someone finish cataloguing the unfinished boxes of uncatalogued letters in the archive basement.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-uninvited-guest.txt"
      ],
      "response_time": 33.08351111412048,
      "retrieval_count": 0,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.8525641025641025,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.6296296296296297,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 1.2857142857142858,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-uninvited-guest.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 33.08351111412048
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What happened at John's wedding?",
      "expected_answer": "His fianc\u00e9e Lila called off the wedding the night before due to panic. John still went to the church, waited, then bought a pretzel and fed it to pigeons.",
      "generated_answer": "John\u2019s wedding never took place. Lila called the night before and told him she couldn\u2019t go\u2014\u201cnot because of you, but because of me.\u201d She hung up, and the next morning the ceremony was empty: guests drifted away, a few relatives offered condolences, and John sat in the front pew with a rose, waiting for a last\u2011minute miracle that never came. He eventually left the church and walked home.",
      "retrieved_docs": [],
      "expected_docs": [
        "the-wedding-that-wasnt.txt"
      ],
      "response_time": 37.6365122795105,
      "retrieval_count": 0,
      "difficulty": "easy",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.08118081180811808,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.1267605633802817,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 2.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 1.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-wedding-that-wasnt.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 37.6365122795105
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 1
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 1.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What was John's philosophy about tragedy throughout his life?",
      "expected_answer": "He saw tragedy as life's way of reminding us that control is an illusion, the universe's way of editing the plot, or simply 'the universe playing bumper cars with reality.' He believed in accepting absurdity rather than fighting it.",
      "generated_answer": "",
      "retrieved_docs": [],
      "expected_docs": [
        "the-absurd-adolescence-of-john-doe.txt",
        "the-calm-storm-of-john-doe.txt",
        "the-uninvited-guest.txt"
      ],
      "response_time": 98.97341561317444,
      "retrieval_count": 0,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-uninvited-guest.txt",
              "the-calm-storm-of-john-doe.txt",
              "the-absurd-adolescence-of-john-doe.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 98.97341561317444
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 3
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    },
    {
      "question": "What jobs did John have throughout his life?",
      "expected_answer": "Junior assistant at Evergreen Public Library, cataloguing assistant at a private rare-book dealer, rare book cataloguer at university archive, librarian, and eventually owner of a secondhand bookstore.",
      "generated_answer": "",
      "retrieved_docs": [],
      "expected_docs": [
        "the-library-of-small-disappointments.txt",
        "the-last-book.txt",
        "the-quiet-clockwork-of-john-doe.txt",
        "the-ordinary-tuesday.txt"
      ],
      "response_time": 101.9722547531128,
      "retrieval_count": 0,
      "difficulty": "hard",
      "evaluation_results": [
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "exact_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "sequence_similarity",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "word_overlap",
          "value": 0.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "answer_length_ratio",
          "value": 0.0,
          "confidence": 0.8,
          "metadata": null
        },
        {
          "evaluator_type": "AnswerEvaluator",
          "metric_name": "keyword_match",
          "value": 0.0,
          "confidence": 0.7,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_average_precision",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "mean_reciprocal_rank",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "document_recall",
          "value": 0.0,
          "confidence": 0.9,
          "metadata": null
        },
        {
          "evaluator_type": "RetrievalEvaluator",
          "metric_name": "context_match",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": {
            "matched_docs": [],
            "expected_docs": [
              "the-quiet-clockwork-of-john-doe.txt",
              "the-ordinary-tuesday.txt",
              "the-last-book.txt",
              "the-library-of-small-disappointments.txt"
            ],
            "retrieved_docs": []
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "response_time_score",
          "value": 0,
          "confidence": 1.0,
          "metadata": {
            "actual_time": 101.9722547531128
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "retrieval_efficiency",
          "value": 0.0,
          "confidence": 0.0,
          "metadata": {
            "retrieved_count": 0,
            "expected_count": 4
          }
        },
        {
          "evaluator_type": "PerformanceEvaluator",
          "metric_name": "answer_provided",
          "value": 0.0,
          "confidence": 1.0,
          "metadata": null
        }
      ]
    }
  ],
  "aggregated_metrics": {
    "total_questions": 15,
    "total_duration": 0,
    "AnswerEvaluator_exact_match_mean": 0.0,
    "AnswerEvaluator_exact_match_min": 0.0,
    "AnswerEvaluator_exact_match_max": 0.0,
    "AnswerEvaluator_exact_match_count": 15,
    "AnswerEvaluator_sequence_similarity_mean": 0.29722620611321937,
    "AnswerEvaluator_sequence_similarity_min": 0.0,
    "AnswerEvaluator_sequence_similarity_max": 0.9090909090909091,
    "AnswerEvaluator_sequence_similarity_count": 15,
    "AnswerEvaluator_word_overlap_mean": 0.27622561201129847,
    "AnswerEvaluator_word_overlap_min": 0.0,
    "AnswerEvaluator_word_overlap_max": 0.8125,
    "AnswerEvaluator_word_overlap_count": 15,
    "AnswerEvaluator_answer_length_ratio_mean": 1.1226266475460025,
    "AnswerEvaluator_answer_length_ratio_min": 0.0,
    "AnswerEvaluator_answer_length_ratio_max": 2.0,
    "AnswerEvaluator_answer_length_ratio_count": 15,
    "AnswerEvaluator_keyword_match_mean": 0.8,
    "AnswerEvaluator_keyword_match_min": 0.0,
    "AnswerEvaluator_keyword_match_max": 1.0,
    "AnswerEvaluator_keyword_match_count": 15,
    "RetrievalEvaluator_mean_average_precision_mean": 0.0,
    "RetrievalEvaluator_mean_average_precision_min": 0.0,
    "RetrievalEvaluator_mean_average_precision_max": 0.0,
    "RetrievalEvaluator_mean_average_precision_count": 15,
    "RetrievalEvaluator_mean_reciprocal_rank_mean": 0.0,
    "RetrievalEvaluator_mean_reciprocal_rank_min": 0.0,
    "RetrievalEvaluator_mean_reciprocal_rank_max": 0.0,
    "RetrievalEvaluator_mean_reciprocal_rank_count": 15,
    "RetrievalEvaluator_document_recall_mean": 0.0,
    "RetrievalEvaluator_document_recall_min": 0.0,
    "RetrievalEvaluator_document_recall_max": 0.0,
    "RetrievalEvaluator_document_recall_count": 15,
    "RetrievalEvaluator_context_match_mean": 0.0,
    "RetrievalEvaluator_context_match_min": 0.0,
    "RetrievalEvaluator_context_match_max": 0.0,
    "RetrievalEvaluator_context_match_count": 15,
    "PerformanceEvaluator_response_time_score_mean": 0.0,
    "PerformanceEvaluator_response_time_score_min": 0,
    "PerformanceEvaluator_response_time_score_max": 0,
    "PerformanceEvaluator_response_time_score_count": 15,
    "PerformanceEvaluator_retrieval_efficiency_mean": 0.0,
    "PerformanceEvaluator_retrieval_efficiency_min": 0.0,
    "PerformanceEvaluator_retrieval_efficiency_max": 0.0,
    "PerformanceEvaluator_retrieval_efficiency_count": 15,
    "PerformanceEvaluator_answer_provided_mean": 0.8,
    "PerformanceEvaluator_answer_provided_min": 0.0,
    "PerformanceEvaluator_answer_provided_max": 1.0,
    "PerformanceEvaluator_answer_provided_count": 15,
    "success_rate": 0.8,
    "avg_response_time": 49.704434315363564
  }
}